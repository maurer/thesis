\chapter{Holmes}
% Overview
\section{Overview}
% * Binary analysis information as facts in a deductive database
The general approach of \sysname\ is to view information about the binary as facts in a deductive database, and analyses as logic-language style rules used to derive additional facts.
% * Traditional logic language approaches treated the logic language as an analysis module in a larger scheme
Logic language as used in binary analysis has traditionally been relegated to a role of implementing a particular analysis, to be later integrated by custom code\todo{cite}.
% * Use the logic language to integrate other analyses, treating external code as where-clause-functions (analogous to external predicates)
Instead, I seek to use logic language as the medium for integration, treating traditional analyses in a manner similar to external predicates.
I expect to see benefits in the form of explicit provenance, ease of integrating new analyses, and clarity of what forms of reasoning are in use \todo{forwardref to example on call-cc reasoning in cfg recovery / value analysis}.
However, existing logic languages have limitations which make them less than ideal for this purpose.
% * Use monotonic aggregators (secref) to enable functions to consume 'all' of something monotonically
In order to express an analysis wanting to use information from many facts of a given kind, I introduce monotonic aggregators\todo{forward secref}.
% * Use hypothetical circumscription (secref) to enable functions to consume 'all' of something in a non-monotonic way, while still allowing the database to maintain a monotonic view
For those analyses which intend to use aggregated information in a non-monotonic way (such as knowing that a fact has not yet been discovered), I introduce hypothetical circumscription\todo{forward secref}.
% * Use explicit backwards chaining to enable expensive or nonterminating functions (esp. fuzzing) (secref)
To deal with an environment in which there are both analyses which run quickly and have nearly-always-used result, and those which are potentially expensive to run and will only be relevant to certain queries, \sysname\ mixes forwards and backwards chaining rules\todo{forward secref}.
% * Implement system in a way that is efficient and will scale to large datasets (secref)
Finally, as I intend to analyze real binaries with the resulting system, \sysname\ will be built to be efficient and scale to large datasets --- the resulting implementation is intended to be practical.\todo{forward secref}

As I explain the additional features, begin by assuming \sysname\ is Datalog with the key difference that saturation may not be possible due to external predicates which are possibly non-terminating.
This gives rise to a notion that a new fact for any given predicate may appear in the database at any time, and informs the design of the new features.

% Monotonic Aggregators
\section{Monotonic Aggregators}
\label{sec:monagg}
% * Why does external code need this? (explicit example)
Sometimes, we need to describe a property over a group of facts.
For example, consider the statement ``My upper bounds on $\id{x}$ preclude the unsafe inputs to the function $\id{f}$, so $\id{f}(\id{x})$ is safe.''
We can model our initial knowledge in this situation by declaring a predicate $\pred{unsafeInputs}{\cdot, \cdot}$ which gives a relation between functions and a superset of their unsafe inputs, and a predicate $\pred{upperBound}{\cdot, \cdot}$ which gives an upper bound on the values a variable may contain.
We also want to express a rule which gathers together upper bounds to check if together they can rule out all the unsafe inputs.
\todo{Make sure strided intervals are explained before this (in the binary analysis background)}
\begin{gather*}
        \pred{upperBound}{\id{x}, 2[1, 5]} \quad \pred{upperBound}{\id{x}, 4[1, 9]}\\
        \pred{unsafeInputs}{\id{f}, \{3, 9\}}\\
        \brule{\pred{safeCall}{\var{F}, \var{X}}}{\pred{upperBound}{\var{X}, \var{U} \not \cap}, \pred{unsafeInputs}{\var{F}, \var{U}}}
\end{gather*}
At first glance, this kind of operation may seem infeasible.
If we match on each fact, neither upper bound can demonstrate that the unsafe inputs are excluded.
We cannot create and match on an intermediate fact containing the sum knowledge of all upper bounds in the general case, as this intermediate fact would need to change as more upper bounds were added in the future.
However, it is clear that this rule is accessing the available information in a monotonic way: No amount of additional restriction to the upper bound would make the predicate no longer match.
% * Definition (see agda in contributions localfile)

The general idea of a monotonic aggregator is to describe this kind of reasoning while formalizing what requirements are needed to ensure monotonicity.
There are two basic components to a monotonic aggregator: a merge function and a query function.
The aggregation function serves to combine the pieces of information about the same property (the upper bounds in the example).
The query function defines a way in which the aggregated information can be matched against monotonically.
Specifically,
\begin{verbatim}
monotonicAggregator = {
  domain : Set
  null   : domain
  merge  : domain → domain → domain
  params : Set
  query  : params → domain → Set
  -- Order independence (doesn't matter in what order merge happens),
  -- repetition is irrelevant
  order-ind : (xs ys : List domain) --Two lists of domains
            -- All elements of xs are in ys
            → ((x : domain) → (x ∈ xs) → (x ∈ ys))
            -- All elements of ys are in xs
            → ((y : domain) → (y ∈ ys) → (y ∈ xs))
            -- Merging either list results in the same value
            → ((foldl merge null xs) ≈ (foldl merge null ys))
  -- Merging in new information cannot invalidate a match
  monotone : (p : params) → (x y : domain)
           → query p x → query p (merge x y)
}
\end{verbatim}

% * Set-bound examples
As a simple example, consider an aggregator for lower bounds defined as sets.
In this case, the aggregator's null element is the empty set, merge function is union, and query function is subset.
As union is known idempotent, commutative, and associative, it is order independent.
Since a union can only grow the set, if a query set was a subset before a merge, it will be so after as well.

% * Abstract interpretation example (strided intervals)
In the example problem, we dealt with upper bounds defined as strided intervals\todo{backref}.
Here, the aggregator's null element can be defined as the empty set, merge as intersection, and query as non-intersection of a parameter set.
Similarly to union, intersection is well behaved in terms of order independence.
Since intersection can only shrink the strided interval, any set which was non-intersecting before will be non-intersecting afterwards.

% * Lattice connection
It seems likely that most (if not all) instances of monotonic aggregators will relate to a semilattice.
The meet operation corresponds naturally with merge, and the queries I have conceived of thus far all correspond to partial ordering operations on the semilattice.

% Hypothetical Circumscription
\section{Hypothetical Circumscription}
\label{sec:hypcirc}
% * Why do we need this? (CFG example)
Unfortunately, sometimes the reasoning we need to do is fundamentally nonmonotonic.
Following the previous example, we might want to know that a particular input was not ruled out by the upper bound before proceeding (e.g.\ was a member of the upper bound aggregate).
However, this would be non-monotonic, since the upper bound could later rule it out upon discovery of stricter upper bounds.
Since we may never have a final upper bound, we need some way to safely perform this type of reasoning.

A driving example for this feature is control flow graph recovery.
As mentioned earlier\todo{background backref}, this procedure involves explicitly postulating that our current knowledge of control flow is complete, computing potential values, and determining whether our assumptions were violated, updating them if they were.
Enumerating the successors or predecessors of the node (the initial assumption phase) is fundamentally non-monotonic, as it gives us information about which facts are not yet in the database.

% * Definition (reveal domain explicitly)
Initially, we define hypothetical circumscription as accessing an aggregate value from one of the previous aggregators, but with direct access to the value rather than via a query function.
If at any point later, that aggregate value changes, any rules which circumscribed based on it must be re-executed, and if they produce differing results, anything depending on the previous execution dropped.

% * Why is this circumscription? (Minimizing the extent of a given pattern)
It may not be immediately clear why this is circumscription.
Recall that circumscription consists of picking from amongst multiple possible assignments of truth by minimizing the extent of portion of the model\todo{backref to background}.
In this case, we are minimizing the extent of a given match by making the assumption that no new matching facts will be discovered.
% * Why is this hypothetical? (It is possible we will receive future evidence suggesting the minimum extent is larger)
However, since it is expected that in some cases new facts \emph{will} be discovered which change the extent of the match, we need to track these assumptions and consider reasoning off one of these branches to be hypothetical --- it makes the falsefiable hypothesis that the circumscription will not be contradicted.

% * Why does revealing the domain constitute this?
Why does revealing the aggregate constitute precisely the power of circumscription over a given match?
%   * At least as powerful because we could use list union + any query + this to get explicit membership
It is at least as powerful since we could define an aggregator adding elements to a list, reveal the aggregate and so extract the exact current membership of the database for a given match.
%   * At most as powerful because we could fold over membership to produce the given domain
It is at most as powerful because we could manually fold the merge operation over the membership results of circumscription, acquiring the aggregate.
%   * It could be more powerful if order-indep wasn't a property on aggregators
As a side note, it could be more powerful if we did not require the merge order independence property --- then it could potentially be relevant in what order the database had arrived at each conclusion, and it would not be possible to replicate the aggregate just by knowing the membership of the database.
% * Call-CC
%   * Why do we need this extension? (CFG->CFG example)
%   * Simplified 1,2,3 -> 1,2,3,neg4 -> 1,2,3,neg4,4 -> 1,2,3,4 example
%   * Why is this call-CC?
%   * Why can we do "bag" circumscription (e.g. circumscribe neg4 and neg5 "simultaneously" and pick one to break out on)

% Bidirectional Proof Search
% * Forwards chaining = datalog
%   * Want this to represent things which
%     * Almost always need to be done
%     * Can be done efficiently
%     * Have a reasonably bounded extent
%   * Good for preprocessing/directed auto-analysis
%   * Examples
%     * Parse
%     * Lifting
%     * Initial CFG
%     * Loaded memory image
% * Backwards chaining = prolog
%   * Want this to represent things which
%     * Rarely need to be done
%     * Have an enormous or infinite extent
%     * Are potentially expensive
%   * Good for directed queries from a user
%   * Examples
%     * Fuzz
%     * Solve this function for stack overflow with symex + smt
%     * TODO more examples
% * Execution strategy
%   * Queries / rule firing acts as a transaction, multiple may be in flight
%   * Try to fire any legal forwards chaing rules until quiescence
%   * If a query is received
%     * Output known matches
%     * Do a backwards search (strategy tbd), treating forwards rules as backwards rules
%     * Commit results of backwards search to database and output

% Implementation Strategy
% * Lang = rust
%   * Safe
%   * Efficiently handles large data
%   * Threading story
% * Database engine = postgres
%   * Transactions (multiple rules in flight)
%   * Joins (avoid writing a custom query compiler for match terms, just compile to joins)
%   * Efficient jsonb support for custom user data structures
% * RPC system = Cap'n'proto
%   * Efficiently handles large data
%   * Encoding of structured data for user types
%   * Supports "sturdy refs" for resuming analysis services
%   * Multilang support (python, rust, C++, js, partial ocaml)
% * See impl in progress at my github
