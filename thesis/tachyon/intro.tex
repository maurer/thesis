\section{Introduction}
\label{tach:sec:intro}

Most attacks target known vulnerabilities for which there are already
patches.
For example, Microsoft reported that only 0.12% of all exploit
activity in the first half of 2011 involved a zero-day attack for
which a patch has been available for a month or less~\cite{msir:2011}.
For the other 99.88\%,
exploits were successful simply because available patches were not
installed. 
These statistics indicate that one of the best ways to reduce
security incidents due to exploits is to simply patch vulnerable
systems.

The need to rapidly deploy security patches in enterprise environments
is hampered by the need to also test patches for problems. Bad patches
often have more business risk than a security breach, suggesting that
the ability to test patches and guard against such problems might
result in faster deployment of security patches. 
Measures currently deployed in cloud environments deal with random
failures, rather than systematic ones caused by bad patches.
As a result, current
best practices amount to manual testing, which is slow, error-prone,
and expensive. For example, NIST best practices recommend manual patch
testing (which is slow) on pre-production environments (which are
expensive to acquire and maintain) when available, or simply waiting
to see if others report a problem or  not~\cite{nistpatches:2005}.
While such approaches prevent bad patches from being applied, they
increase the vulnerability window.
Even when a pre-production environment is provided through virtualization, reducing the cost substantially,
auxiliary services, such as databases, must be simulated. This leads to excessive administration overhead and compute overhead.
Additionally, effects are captured
in an often ad-hoc manner (e.g. by recording network traces), which can miss changes the administrator did not think to look for.
 For example, the US Air Force
implements a centralized patch testing procedure for their half million
managed machines, but as a result, delay patch rollout by up to a
quarter year~\cite{afpatches:2007}.

% and that enterprises spend additional money for pre-production
% environments 
% In a perfect world, NIST also recommends an enterprise have a
% non-production environment for testing patches that accurately
% replicates the production environment~\cite{nistpatches:2005}.
% Lacking a non-production environment, NIST advises that system
% administrator wait for others to install the patch, and then ask the
% others whether the patch broke anything. Both approaches are error
% prone, and depend on introduced bugs being caught in normal use.

% Thus, on the one hand we know installing patches faster would reduce
% the vulnerability window and number of successfully compromised
% systems. On the other, we are told to test patches before installing
% them.  The result is patches do not get quickly deployed on vulnerable
% end hosts. For example, the US Air Force tests patches centrally, but
% is only rolls out new patches to their half million managed machines
% quarterly~\cite{afpatches:2007}.

If we could automatically test patches, then we could shorten the
vulnerable time window between when a patch is released until it
is installed. However, automated patch testing faces several challenges.

First, in order to faithfully check that functionality is preserved in
a patch, we should be able to test a patch on the system it will
ultimately protect.  Second, in order to be widely applicable, we
should be able to test patches in the common scenario where the patch
is a new binary program, as source is often not available. Third, we
want to minimize manual effort. As patches can change the semantics of
a program, a human will likely always need to be in the loop to
determine if the semantic changes are meaningful. However, we still
wish to automate the system as much as possible.  Unfortunately, there
is very little work on automated patch testing, and no previous work
addresses all these requirements.

% Manual patch testing tends to be error prone. This is as a result of
% dangerous or incorrectly implemented program paths frequently being
% edge cases or otherwise improbable inputs. While testers may do their
% best, in the end they are just re-vetting the patch the same way the
% software firm did, and are unlikely to detect all errors.

In this paper we propose the first techniques for live patch testing
via tandem execution. Our insight is that current manual testing
checks whether the executions of a pre-patch and post-patch binary
produce different outputs on known inputs.  We call this
\emph{observational equivalence} between pre and post-patch.

Tandem execution uses this insight to automate patch testing by
simultaneously running both programs on the same input.  More
specifically, in tandem execution one program runs live on the
system (e.g., the patched program), with all system calls (syscalls)
being serviced by the kernel. The second version of the program (e.g.,
the unpatched program) runs in tandem, but with each syscall to the
kernel simulated by replaying the side effects from the corresponding
calls of the live version.  The replay prevents duplicating
side-effects, such as writing to the same file twice.

If the two programs deviate on the syscalls issued, or the arguments to
syscalls, then  they are not observationally equivalent.
We record the deviation and inform the user of the potential problem.
At this point, the actions that can be taken are to halt the program,
or to specify that the deviation is permitted.  In order to continue
testing, the user provides a rewrite rule that specifies how to handle the
deviation, and automated patch testing continues. In our experiments
we show that rewrite rules are small when needed, and often
completely unnecessary for security-related fixes.


We implement our approach in a system called \tachyon. \tachyon is
based upon syscall replay techniques for binary programs, but with a
new twist.  Existing system call replay schemes are designed to record
system calls from one run of a binary for replay against \emph{exactly
  the same binary}, e.g., \cite{undodb,patil:2010,guo:2008}.  Since
both record and replay are against the same binary, the record step
only needs to conceptually keep a snapshot of the memory cells
affected by the syscall.  The affected memory cells are typically
determined by differencing the pre and post-syscall memory state. During
replay, the memory cells at exactly the same addresses are replayed
with the recorded data.

The twist in our setting is we want to replay syscalls to a
\emph{different binary}.  Typically the two binaries will have a
different memory layout, and may make different syscalls. For
example, the system may have ASLR enabled, or a patch may change a
buffer from the too-small size of 1024 bytes to the just-right size of
4096. In either case, all pointers are likely to have different
addresses between the patched and unpatched versions. As a result,
previous raw memory snapshot and replay approaches do not work.


To address our twist, \tachyon takes a semantic-based approach to
replay only the semantically meaningful information from a syscall
in a recording during replay, rather than capturing details like
pointer values which may change between patches.  Our approach is
enabled by three techniques.  First, we extend the C type system to
include a full description of the syscall side-effects.  The
description enables \tachyon to identify semantically meaningful
arguments and results in syscalls instead of relying on blind
memory differencing.  The work to annotate the system calls needs to be
performed once, and can be reused for all programs.  Second, \tachyon
utilizes a rewrite rule system to compare syscall sequences for
equivalence and rewrite if necessary. The rewrite system gives the
end-user the ability to say when deviations are permitted in a
systematic manner.  Third, \tachyon uses syscall interposition
techniques to record the effects of syscalls on a live program, and
replay those effects to simulate syscalls on the tandem program.



% Once a deviation is detected, the administrator has two options. One
% option is to continue running just the patched program and debug the
% deviation later. Another is to use the rewriting system described in
% \S~\ref{tach:sec:equiv} to accommodate the change and continue testing.


Tandem execution makes patch testing in new scenarios possible.
For
example, a test administrator can run the pre-patch binary live and the
post-patch in-tandem on the same system.  Any deviation reported is
either (a) a bug in the patch, or (b) an exploit against the buggy
program that is averted in the patched program, or (c) a permitted
change that changes IO behavior.  In all cases, the administrator
should be informed of the deviation.  Alternately, a
security-conscious administrator may run the patched version live,
noting deviations with the pre-patched version.  In either case,
the tandem execution achieves live patch testing without duplicating
side-effects. The closest current best practices come to similar
results requires mirroring a production environment with a
pre-production environment, which is expensive and requires
significant effort to maintain.  We discuss other possible
applications such as creating honeypots in \S~\ref{tach:sec:discussion}.



We have implemented and evaluated \tachyon on a number of security
patches, and demonstrated that our techniques can successfully detect
deviations.  We have also performed micro-benchmarks that show 
our implementation is efficient with respect to the amount of I/O
performed. We show our implementation records full syscall
information faster than \texttt{strace}, a tracing tool targeted at
binary programs, and is efficient in comparison to an untraced run.

\paragraph{Contributions.}
The main contribution of this paper is techniques for
live tandem execution for patch testing. These techniques automate
a large part of patch testing, thus reducing the vulnerability window for
unpatched systems.  In particular:
\begin{itemize}
\item We are the first to propose techniques for automated
  patch testing that address all the above challenges;
  we have implemented them in \tachyon. We demonstrate where we run both the
  unpatched and
  patched binary and use tandem execution to detect deviations. An
  additional benefit of tandem execution is that it can utilize
  extra cores for the security purpose of patch testing.

\item We develop a type system that fully encapsulates the side
  effects of syscalls necessary for replay. Our type system is similar
  to~\cite{guo:2008}, but does not require source code access or
  explicit developer cooperation.

\item We propose a light-weight rule-based system for checking syscall
  stream equivalence (and rewriting if necessary) when the sequence of
  syscalls between the patched and unpatched binary are not exactly
  the same.

\item We have implemented our techniques in \tachyon using Haskell (a type-safe
  language) and validated the techniques experimentally.  Our
  system is robust enough to handle single-threaded and
  multi-threaded programs. We evaluate our approach on several
  real-world patches, as well as synthetic benchmarks, to show the
  effectiveness and performance of \tachyon.


\end{itemize}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "paper"
%%% End: 
