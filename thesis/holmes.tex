\chapter{Holmes}
% We've argued why datalog, now argue why we want these extra features
Holmes is a dialect of datalog, tailored with extensions for the specific use case of analyzing compiled code.
Specifically, a normal dialect of datalog will fall short on several tools desired by the analysis author:
\begin{itemize}
		%TODO expand list
	\item Data structures
	\item Aggregation
	\item Negation
\end{itemize}

\section{Feature Selection}
To address these lacks, we add a few features to base Datalog.
%TODO once expanded list is there, do short "X address Y and Z, Q addresses..." blurb
\subsection{Callbacks}
%TODO jam widening in dataflow somewhere in here as a justification?
% Why do we need External Predicates
%TODO rewrite condition - "computation" too vague
Tasks which do not involve a fixpoint, but do involve computation, can frequently be both more difficult and more expensive to write in pure datalog.
For example, parsing an ELF and splitting it up into segments, sections, generating records for its symbols, etc. could in principle be written in datalog.
However, this would be difficult to write (operating on a string as a linked list, or similar structure), slow to compute due to high arity joins, and would require that the input first be transformed before even entering the program.
Other similar examples include lifting (translating a sequence of bytes into a semantics IL), concrete execution, and arithmetic.

Previous approaches have noted that many of these steps come towards the beginning of analysis, and perform these tasks as a precompute phase before handing the results to datalog to process.
In our case, we are trying specifically to avoid such phasing.
The lifter might be needed again for previously undiscovered code.
The loader might be needed again if we discover a call to \texttt{execve} and wish to follow it.
Doing a phased datalog prevents the easy interleaving of these functionalities into the global fixpoint.

While datalog predicates can be a great data structure, they are not always the best data structure for all tasks.
Datalog predicates can effectively be viewed as an append-only, deduplicated, index-by-anything table.
While this is a great data structure for many tasks, some concepts are better represented in other ways.
One example is ILs and ASTs.
As frequently nested, branched structures, they \emph{can} be represented in datalog, but walking one would take a very large number of lookups compared to using a traditional ADT approach, not to mention the clumsiness.
Other similar concepts include formulae (as in SMT) and any kind of variable-length buffer representation.
All of these can be done in pure datalog, assuming appropriate preprocessing has been done.
However, the resulting time and space costs make this something to be avoided.

To address the above two, we add the ability to register a callback to a datalog rule.
If specified, whenever a that rule fires, the corresponding callback will be used supplementarily to determine the values to substitute into the head.
This allows use of traditional functional or imperative style code to implement data structure transformations or perform operations which would be slow to do in the base datalog.
Additionally, it allows us to more readily incorporate existing code (such as the BAP~\cite{bap} lifter) rather than rewriting it from scratch.

This is equivalently powerful to external predicates in other languages in terms of expressivity.
Any callback specified could instead be turned into an external predicate and simply appended to the query.
A query involving external predicates might need to be split up into phases to be expressed in callbacks.
If an output variable of an external predicate is present in another term in the query, one would need to use intermediate tables, as the callback only occurs at the \emph{end} of a query, and there can only be one per query.
This simplifies the design of the datalog engine (the join engine is entirely separate from the callbacks), at the cost of the ability for a sufficiently advanced engine to better optimize such queries.

\subsection{Monotonic Aggregation}
Traditionally in datalog, your match clause may only access a fixed (though arbitrary) number of facts at the same time.
Even counting can be difficult.
To verify that there are at least three instances of some predicate p, you would normally write:
\begin{verbatim}
p(x) & p(y) & p(z) & neq(x, y) & neq(y, z) & neq(x, y)
\end{verbatim}
The size of this query grows as $n^2$ in the number of elements you are counting.

This same difficulty occurs when encoding a dataflow or abstract interpretation algorithm into datalog.
When two branches come together, a new fact representing the state with the meet applied needs to be generated.
If we do this naively, simply matching on the existence of two states at that program point and generating a new one by merging, we will again go rapidly superlinear.

In existing systems~\cite{doop1} this is dealt with by ensuring the state in question can be extended simply by adding more facts.
When possible, this works nicely, but it prevents the use of things like strided intervals~\cite{vsa} or widening operators in dataflow algorithms which lack finite descending chains.
This is because all of these situations require reasoning about a variable sized subset of the data to make their conclusion, not just a fixed window.

Finally, in the case of employing an external solver, they often need to receive all the inputs up front, rather than incrementally.
Calling out to an SMT solver will not work if your formula is stored as facts in a datalog representation; you would first have to walk them with a rule and a callback (or a rule and an external predicate in another system) to build up a viable representation and hand it off.
The same is true even of simpler concepts, like applying Steensgaard's algorithm~\cite{steensgaard-alias} to a set of constraints - it will need all of them, or you will end up storing incremental program states in your database as well, and end up back at the $n^2$ problem.

Traditionally, this is dealt with by appling a postprocessing step to the datalog computation.
After it is done, a query is run, and the aggregation is performed by an outside-of-datalog program.
As stated earlier though, we want all portions of the analysis to be able to trigger all others to avoid explicit phasing.

Some of these specific scenarios can be worked around with tricks, but they do not apply universally and are still awkward to write.
For example, a more clever author of the counting check might instead use a greater than operator instead of not-equals, assuming that field is ordered.
The resulting query would then only be linear in the count you wanted to check against.
However, it would still not be possible to have a variable limit to check against without significant complexity.

To address these issues, we allow for predicates with aggregation.
If a predicate is declared with aggregation, a provided meet operator will be used to aggregate submissions to each aggregate field for which all non-aggregate fields match.
This addition allows us to address each of these issues.
In the case of counting, we simply use set-union as our meet operator.
For dataflow or abstract interpretations, we can have parameters like program location be non-aggregate fields, while the state is an aggregated one.
Programs using this feature need to be aware that they may receive the aggregation of any subset of derived facts, and are only gauranteed to ever receive the aggregation at the fixpoint.
%TODO add forwardref here for why

\subsection{Hypothetical Circumscription}
Some questions revolve more around what isn't there than what is.
For instance, if \texttt{ud2}\footnote{
LLVM inserts this instruction to denote unreachable code, and is intended to cause a trap if hit
} is found in the binary, we might wish to determine if it is in fact statically unreachable.
This requires us to be able to state that we know \emph{all} of the edges entering that basic block, not some subset.

As a more concrete application, if we have an algorithm which works on the SSA representation of a function, creating an SSA representation of that function requires the entire control flow graph.
If we add edges later, conclusions derived from the incomplete SSA form might become incorrect.

Traditional datalog either disallows negation, or allows it through explicit stratification.
In stratified negation, each predicate belongs to a ``strata''.
The database is computed by sequentially adding the rules that have heads in each of the new strata in sequence.
Since all the rules which could generate a particular predicate will not be used again, we can assume (using negation-as-failure) the negation of any fact not yet in the database at that point.

Not all programs are written such that stratification is possible.
Consider a graph with vertices for each predicate, and a edges for a predicate which may be derived from another predicate.
Mark any edges for which the body-side predicate was negated as negated.
If this graph has no cycles which include negation, then the program can be stratified.
Contract strongly connected components on this graph to a single vertex, and then output in topographically sorted order to generate a stratification.

In the context of doing program analysis on binaries, we might wish to avoid this even when reasoning purely monotonically.
Consider an analysis which determines whether a function will never return.
This information is important in analysis of a calling function because it should not expect control to proceed past the called function.
To declare that a function will never return when called, we must know \emph{all} the paths within it, not just some of them.
As a result, we are implicitly talking about knowing the negation of additional edges in the control flow graph.
If we employed stratified negation, we would have to close over the entire control flow graph at once, leaving us unable to employ this information in the CFG definition for a calling function.

To address this need, we add hypothetical circumscription.
The core concept is that we can at need assume that a particular chunk of information is expanded, and reason forwards.
In the event that this turns out to be false, we can retract that assumption, and reason forwards again.
This allows us to deal with cases of negation which are not trivialy stratified.
In the language, this feature is implemented as the ability to match on an aggregation and know your rule has received only the aggregation which will be present in the final fixpoint.
Contrast that with the monotonic aggregation, where your rule must have correct operation for any subset of the possible aggregations along the way.
This can be used to implement the stratified case in a straightforwards manner, and also to support dynamic negation as describe in the never-returning example above.

\subsection{Call/CC}
\label{sec:motive-callcc}
The astute reader may have become worried at the end of that last subsection, as it is not without reason that the standard approach to negation involves stratification.
In the case that we have a negated inference cycle not just on predicates, as stratification prohibits, but on the actual facts, the approach as described so far would lead to alternation and indeterminacy.

Interestingly, there is actually a use case for allowing negated cycles in program analysis.
In the case of the outputs of an dataflow analysis or a control flow recovery, we will need to circumscribe over their results in order to know we have received the actual output.
Using an incomplete run of an alias analysis, for example, would result in too-small upper bounds being used.
However, due to the interconnectedness of these examples, the complete alias analysis information might alter the control flow graph by refining the information used to determine the target of indirect jumps.
Changing the control flow graph would in turn invalidate the circumscribed alias analysis.

Strictly looking at the system thus far, this would loop.
Altering the control flow graph would retract the alias analysis, which would retract the alteration to the control flow graph.
Thinking about what a human would do if they had gone down the same reasoning path points to a potential solution.
If you had assumed you saw the totality of control flow, and from that, came to find a new control flow edge, you would assume that your initial assumption must have been wrong, and that edge really is there.
Essentially, you would assume $(\neg P \rightarrow P) \rightarrow P$.

This matches the type signature of \texttt{call/cc}, and not without reason.
In this case, the continuation is the reasoning strategy forwards, assuming P can be determined to be true.
If P is not determinable to be true, this continuation cannot actually be invoked, and we never go down that path.
If P can be determined to be true, in a traditional programming language we might go down that path.
In our case, we are constantly watching for P to be determined to be true, and if it is, we immediately take the continuation.

This feature is invisible to the user other than for performance characteristics.
The user need only specify their rules as usual, circumscribing over things which need to be complete, and if this \texttt{call/cc} condition arises, it will be automatically dealt with by reasoning forwards from the new (expanded) circumscription, after retracting relevant other derivations.

\section{Informal Semantics}
%TODO review and rewrite this whole chunk, it was written before I decided it was informal, so it mixes the formal and the informal
Before giving a more formal treatment of what outputs are correct for a given Holmes program, we describe how each feature functions informally, as you might see in a programmer's guide.
Afterwards, there is a simple traced execution of a toy program which employs all of these features.

\subsection{Callbacks}
External predicates are traditionally expressed as predicates in the language, equipped with modes for each field and a function which transforms from the input fields to some representation of the output fields.
This model can add some complexity to both the inference engine and writing code in the language.
If an external predicate is present in the match, how should it be searched?
There are several potential strategies, and complexity only increases when there are two external predicates present.
From the programmer's point of view, the monodirectionality of the predicate is concealed, which can lead to surprise when a predicate can't be used the same way as others.

To address this, I explicitly phase the application of rules.
First, there is an initial match phase that works as usual.
Then, optionally, the rule may have a function which takes as input a set of assignments to variables in the match clause, and gives back a list of assignments to any variables present in the head which were not defined in the match.
This list may be empty, and this result indicates match failure.

For example, in the rule
\begin{verbatim}
simple_func: p(y) <- q(x) & r(x) + func
\end{verbatim}
\texttt{func} would be expected to take in the value of x, and return a list of values for y.

\subsection{Monotonic Aggregation}
Monotonic aggregation is defined as a property of a subset of predicate fields.
A result is legal in a match against an aggregated predicate if there exists a specific subset of derivable facts which match via equality on all non-aggregation fields, and for whom applying the aggregation function provided to the remaining fields would produce the result.
A result is mandatory if the selected subset is the largest possible for some indices.
For a predicate \texttt{p(i32, i32, IntSet\^{}union, i64\^{}max)}, an attempt to match against \texttt{p(a, b, c, d)} with a database \texttt{p(1, 2, {3}, 4), p(1, 2, {4}, 3), p(1, 1, {}, 7)} would be \texttt{p(1, 2, {3, 4}, 4), p(1, 1, {}, 7)}.

%TODO finish section
%TODO: mention reason for not using a lattice initialism is that there would be a potentially infinite number of matches that produce it, so matching against a predicate with aggregation whose indices were not fully constrained by the rest of the match clause would produce an infinite number of results in the case of non-finite domains
\subsection{Hypothetical Circumscription}
\label{sec:inf-circ}
Hypothetical circumscription extends monotonic aggregation by allowing the rule to receive only the largest subset.

This is a form of negation.
To illustrate, consider the program
\begin{verbatim}
p(unit)
neg_p(unit)
q(bool^and)

q_init: q(true) <-
pq: q(false) <- p(())
neg: neg_p(()) <- ~q(true)
\end{verbatim}

The predicate \texttt{neg_p} now contains the negation of \texttt{p}.
This formulation can be extended in a straightforwards way for predicates whose indices have finite domain:

\begin{verbatim}
p(finite_type)
neg_p(finite_type)
q(finite_type, bool^and)

q_init_i: q(x_i, true) <- 
pq: q(x, false) <- p(x)
neg: neg_p(x) <- ~q(x, true)
\end{verbatim}
where \texttt{q_init_i} is reproduced for every $x_i$ in \texttt{finite_type}

The intuitive reason for this connection is that knowing the largest possible aggregation also entails the knowledge that all of the other possible members of the aggregation cannot be derived.
In the case of finite domain indices, this allows us to negate the predicate in the way described above, since we can subtract the aggregation returned form the universal aggregation.

When the domain of an index is infinite, we could still construct the negation through use of an external function capable of enumerating the index values, e.g.
\begin{verbatim}
q_init: q(x_0, true) <-
q_step: q(x', true) <- q(x, true) + succ
\end{verbatim}
however, the resulting database would be infinite, so this would really only have solid meaning under a minimal model interpretation.

Since I am adding a construct with the power of negation, a natural question is how I will deal with inconsistency.
The traditional approach here is to stage computation, requiring a predicate to saturate before its negation can be considered.
For the call/cc feature next, and to deal with new facts (e.g. from a concrete program execution) added to the database after inference has begun, I can't actually use this stratification.

In an intuitive form, my solution here is to consider all negation as hypothetical, and consider answers from as far along a consistent hypothetical tree as possible.

More formally, I describe this relation using kripke semantics.
Initially, I will deal only with finite-domain indices.
When dealing with finite domain indices, we can rewrite (however inefficiently) every rule requiring circumscription to one making use of negation.
Specifically, for each rule containing a circumscription in the match, for all aggregated variables, for all values in their domain, write the rule with the variable substituted for the value and appended with the negation of all other values.
%TODO proof sketch of the equivalence.
Chaining this with the rewriting of aggregation into a less efficient form without aggregation, and using the finite domain property to reduce external functions to tables, I can reason about datalog with negation here.
Specifically, assuming a finite domain for all index variables, all extensions described so far can be reduced to basic datalog with the added ability to negate clauses in the body of a rule.
Note that negated heads are not required here, and so are not under consideration.

Define a candidate world $\omega$ to be a tuple of a context $\Gamma$ containing program rules and some set of negated facts $N$.
Define $\omega \models P$ to mean that $P$ is in the minimal model of $\Gamma$, interpreting the matching of negated facts to only be able to match those facts explicitly in $N$.
Define the candidate accessibility relation $\omega \cac \omega'$ to mean that $\omega'$ is $\omega$ with its $N$ augmented with a $P$ not in $N$, which is present in the right hand side of some rule in $\Gamma$.

Define a world to be a candidate world in which $\omega \models P \imp \omega \not \models \neg P$, $\models$ to be as in the candidate case, and the accessibility relation $\leq$ to be the subset of $\cac$ which is only between worlds at this level, augmented with reflexivity.
Additionally, define $w \vdash P$ to mean either $w \models P$, or in the case of $P = A \rightarrow B$, $\forall w'. w \leq w', w' \vdash A \imp w' \vdash B$.
This forms an intuitionistic Kripke frame.
By definition of the upgrade from candidate world to world, $w \not \models \bot$.
By construction of $\cac$, the accessibility relation leads only to worlds where more things are in the context, and since the only non-monotonic operation in the logic is circumscription, which is excluded at the $\models$ layer, $(w \leq w') \imp (w \models P \imp w' \models P)$.

\subsection{call/cc}
\section{Formal Semantics}
I give the semantics of the Holmes language in two pieces.
The first piece is a description of which fact sets a correct implementation may output as the program result (\S~\ref{sec:allowed}).
The second piece describes the search strategy (\S~\ref{sec:search}) which is concretely implemented by Mycroft (\S~\ref{sec:mycroft}).
I then seek to connect the search strategy to the supported outputs, showing that the search strategy will always make progress (\S~\ref{sec:progress}), only output facts which are supported by the program (\S~\ref{sec:soundness}), and if the correct output is always finite, then the search strategy will terminate (\S~\ref{sec:term}).
%TODO better subsec name
\subsection{Supported Outputs}
\label{sec:allowed}
We begin with negation-free datalog, using minimal model approach.
A model $M$ consists of a universe $U$, an interpretation $I$ which maps ground terms in the program to elements of the universe, and an interpretation $P$ for some predicates $P$, which are each sets containing elements of the universe $U$.
A statement $P(x)$ is true in $M$ if $I(x) \in P$.
%TODO is \Pi the right symbol here?
We say $M$ is a model of a program $\Pi$ if $M$ holds true all EDB facts in $\Pi$, and for all rules whose body clauses are held true by $M$, their heads are held true by $M$ as well.
%TODO see if I can find someone to cite for this stuff?
$M$ is a minimal model if there are no selection of $P' \subseteq P$ (with at least one proper subset) such that a $M'$ using those predicate interpretations instead is still a model of $P$.
To restrict ourselves to a single, canonical model, we consider only those models for whom the interpretation $I$ is the identity function - the Herbrand models.
This interpretation provides a canonical model equivalent to the fixpoint semantics.

\paragraph{Callbacks.}
To describe callbacks, we augment the model with a set of functions representing them.
Each callback has a type signature $f_M : [U] \rightarrow [[U]]$, where the argument represents the assignment to input arguments, and the output represents the possible assignments to output arguments.
It is now an additional requirement for a model $M$ to model a program $\Pi$ that for any callback $f$ used in $\Pi$, $I^{-1}(f_M([I(x_0), ...,I(x_n)])) = f(x_0, ... x_n)$.
\todo{Describe how function argument naming works/interacts. This is really just a detail, but it should be there eventually}
Finally, when checking whether a model supports a particular rule of a program which additionally uses a callback $f$, we now say that if the body clause holds in $M$ for some substitution, then for each element of the list $f_M(u_0, ... u_n)$ where $u_i$ are taken from that substitution, the head clause holds under both the substitution described by the body clause and that described by the function.

%TODO argue this statement in the infinite case.
At this point, we still have a single canonical model, though it may now be infinite.
\paragraph{Aggregation.}
To be a well formed program, any meet operators used in $\Pi$ must define a lattice.
We add to the model additionally a set of meet operators with signature $\wedge_M : (U, U) \rightarrow U$.
For $M$ to model $\Pi$, the meet operators in $\Pi$ and $M$ must hold equal under $I$ translation, as in the callbacks section.
For every predicate $P$ with non-aggregate fields $x_0$ through $x_m$, aggregate fields from $a_0$ through $a_n$, and meet operators $\wedge_0$ thorugh $\wedge_n$, we perform a translation on $\Pi$.
Augment $\Pi$ with the rule
\[P(x_0, ..., x_m, \wedge_0(a_0, b_0), \wedge_n(a_n, b_n)) \leftarrow P(x_0, ..., x_m, a_0, ..., a_n) \& P(x_0, ..., x_m, b_0, ..., b_n)\]
Now $M$ can be checked against our translated program in the same way as before.

The above only forms the upper bounds of what programmers are allowed to expect however.
As aggregation is primarily intended as a performance feature, we need to perform further translation in order to loosen the restrictions to allow use of only the biggest aggregation by the evaluation engine.
%TODO check ordering, meet-join nomenclature, etc
We split each aggregated predicate $P$ into two versions, $P$ and $P'$.
Other than in the augmented rule, anywhere $P$ is used in the body position, it is translated to $P'$.
We add an additional predicate, $P_a$.
$P_a$ is not allowed to be subset for purposes of minimality checking, and represents the choice available to the engine in aggregation disclosure.
We now add the rule
\[P'(\vec{x}, \vec{a}) \leftarrow P(\vec{x}, \vec{a}) \& P_a(\vec{x}, \vec{a})\]
This effectively allows the model to pick and choose any subset of $P$ to promote to $P'$.

Finally, we must insist that the biggest $\vec{a}$ for each $\vec{x}$ in $P$ be promoted. 
%TODO turn this into a rule rather than a restriction.
We define $x \leq_i y$ to be $x \wedge_i y = y$ as a shorthand.
We define $\vec{a} \leq \vec{b}$ to be $a_0 \leq_0 b_0 \& ... \& a_n \leq_n b_n$
For this, rather than add logical rules to $\Pi$, we add the constraint for each $P$,
\[
	\forall \vec{x}, \vec{b} | P(\vec{x}, \vec{b}). \exists \vec{a} P'(\vec{x}, \vec{a}) \& \vec{b} \leq \vec{a}
\]

We now have multiple legal minimal models, since we added the element of choice to the engine by allowing it to selectively parts of the aggregation.
\paragraph{Circumscription.}
Before we define circumscription in the general case, we will first give a description of it in the case without aggregation or functions, e.g. as an addition to base datalog.
In addition to base datalog rules, we now allow negated predicate terms to occur in the body of rules.
We also allow negated EDB predicates (i.e. fully ground) in our programs.

We extend the minimal model definition of datalog to deal with these modifications.
Before checking a model against a program, we augment it with a rule for every predicate of the form
\[
	\bot \leftarrow P(\vec{x}) \& \neg P(\vec{x})
\]
If we see $\neg P$ in the body of the rule, it matches iff $\neg P$ is presnet in the EDB.
Note that negated facts cannot be generated in the IDB under these rules.
We then check the model as usual, with the additional constraint that if the predicate $\bot$ is populated, the model is rejected.
We identify the canonical model as the Herbrand minimal model corresponding to the above.

We now develop a kripke structure.
The set of worlds is the set of programs which have a canonical model as described above.
%TODO do I need to rule out reflexivity here? Mandate it? I forget.
For a program $\Pi$ and $\Pi'$, we define the accessibility relation to be: $\Pi \leq Pi'$ iff both the EDB of $\Pi$ is $\subseteq$ those in $\Pi'$.
For base (non-modal) formulae, we say $\Pi \models P$ iff the canonical model of $\Pi$ contains $P$.
We say $\Pi \models \boks P$ iff $\forall \Pi' | \Pi \leq \Pi'. \Pi' \models P$.
We say $\Pi \models \dia P$ iff $\exists \Pi' | \Pi \leq \Pi'. \Pi' \models P$.

We say that a formula $P$ is ``final'' if $\Pi \models \dia \boks P$.
We say that a formula $P$ is ``universal'' if $\Pi \models \boks \dia \boks P$.

In an ideal world, the output of our engine on this would take the form of a classifier for which non-modal formulae are universal.
I have not found an efficient way to compute this set.
%TODO add section to mycroft on why we don't do universal formula, forward reference.
As an approximation, we instead examine a locally-largest set of non-negated coherent \emph{final} facts.
This can be thought of in general as the non-negated portion of the model at some accessible world from the input program, such that no more useful assumptions can be made.
More specifically, we define the output the non-negated portion of a model $M$ for $\Pi'$, where $\Pi \leq \Pi'$, such that $\not \exists \Pi'' \geq \Pi'. \Pi'' \models P \& \Pi' \not \models P$, where $P$ is a non-negated fact.

Our kripke structure is reflexive and transitive, placing it in S4 (preorder).
For ``well behaved'' programs, as described in \S~\ref{sec:inf-circ}, we also follow G (convergence), placing us in S4.2, directed preorder.
This corresponds to the situation where all final facts are universal, giving rise to our approximation of having the engine output final facts for a particular world.
I do not have an efficient way to check for G in the general case.
\todo{Can I say something stronger? I think it may not be possible to compute whether G holds in the general case without first computing at least one final world.}

This formulation is directly related to stable model semantics~\cite{stable-model}.
\todo{Would it be useful to make this connection more formal?}
Specifically, the selection of negated assumptions with which to extend $\Pi$ is isomorphic to the preselection of a stable set.
\todo{Is it really? Does stable set permit the equivalent of positive assumptions?}
Said stable set adds to the program the negation of everything not in the set by rewriting the rules to either drop negated rules which may not match, or drop clauses from negated rules which are now known to match.
In our case, the kripke structure both makes our search procedure easier, and will make it easier to avoid the possibility that a program with no negation in its EDB has no legal output model (via call-cc).
\todo{Mention connection to autoepistemic expansions?}

As described in our informal section\todo{ref}, Holmes implements negation by allowing the user to receive an aggregation with the assurence that it is the \emph{largest} aggregation it will ever receive.
To pivot from our simple model to this primarily requires a shift in what modifications are made to $\Pi$, and an augmentation of the model.
For every predicate which is circumscribed over, add to the model a new predicate $P_c$, and rewrite the rules to reference it instead when circumscribing.
Add to the program the rules
\todo{rename $P_d$ something more sensical}
\[P_c(\vec{x}, \vec{a}) \leftarrow P(\vec{x}, \vec{a}) \& P_d(\vec{x}, \vec{a})\]
\[P_c(x_0, ..., x_m, \wedge_0(a_0, b_0), \wedge_n(a_n, b_n)) \leftarrow P_c(x_0, ..., x_m, a_0, ..., a_n) \& P(x_0, ..., x_m, b_0, ..., b_n)\]
and for i ranging from 0 to n, additional rules
\[\bot \leftarrow P_c(x_0, ... x_m, ..., a_i, ...) \& P_c(x_0, ... x_m, ..., b_i, ...) \& a_i \neq b_i\]
The modification to $\Pi$ which takes place is no longer adding $\neg P$, but rather adding an assumption to one of the $P_d$ predicates.
Otherwise, the transformation is identical.

Essentially what this does is add the ability to use $P_d$ to select an aggregation on $P$ and promote it to $P_c$.
If the aggregation is not the biggest one, or becomes not the biggest one, $\bot$ will be derived.
This rules out any impossible circumscriptions.
The same kripke structure described above gives rise to a stable semantics system again, only this time we are negating potentially infinite sets of facts at a time.
\paragraph{call/cc}
The major difference between this approach to negation and the stable-set approach is our addition of call/cc.
In the stable-set approach, $\neg A \rightarrow A$ has no stable model.
If $A$ is not in the set, $A$ is required for it to become a Herbrand model.
If $A$ is in the set, $A$ is not required to be present, so while it is a Herbrand model, it is not the minimal one.
In our case, per the motivation (\S~\ref{sec:motive-callcc}), we would prefer that this scenario resolve to $A$.
Essentially, with call/cc we attempt to provide a way to produce a meaningful set even when the system itself is unstable.

As with circumscription, we will first describe this modification without the prior features, then extend it to work with them.
We assume negation works as in the first half or the circumscription section.
The only real change we need to make is to our accessibility relation.
Specifically, we want an accessibility relation which allows non-negated assumptions to be added considered for assumption, but only in those cases where their negated form eventually leads to a contradiction.

First, we reform the previous accessibility relation into a directed graph.
\todo{Stop using EDB of and IDB of everywhere, and use $\Pi_I$ and $\Pi_E$ after defining}
$\Pi \leadsto \Pi'$ here means that $\Pi'$ is a world with exactly one more allowed assumption than $\Pi$, essentially a single-step version of $\leq$.
If $\Pi$ and $\Pi'$ have the same rules, and $\Pi'_E = \Pi_E \cup \{\neg P\}$ (and $\neg P \not \in \Pi_E$), then $\Pi \leadsto \Pi'$.
We now define $\leq$ inductively.
$\Pi \leq \Pi$.
If $\Pi_0 \leq \Pi_1$, and $\Pi_1 \leadsto \Pi_2$, then $\Pi_0 \leq \Pi_2$.

To add support for call-cc, and unstable sets in general, we have two basic scenarios that $\leadsto$ needs to be modified for.
The uncovered cases here always start with, for some $P$, $\Pi$ with $\neg P$ appended is not a world, because $\Pi$ with $\neg P$ is able to derive $\bot$, and $\Pi \not \models P$.
In the first scenario, $\Pi$ with $P$ forms a legal $\Pi'$, and we want to add $\Pi \leadsto \Pi'$ for that instead.
In the second scenario, neither $P$ nor $\neg P$ may be legally added to $\Pi$, yet we do not have that $\Pi$ models either of them.
Essentially, $P$ is \emph{inconsistent}.
While there are some interesting logics involving inconsistency\todo{citations}, it is not an intended feature of Holmes.
To deal with this, we preclude programs which have this property from being worlds.
Note that the input program will always be safe here.
It has no negated terms in the head, nor is a negated fact allowed in the EDB.
As a result, without first having made a negated assumption, $\bot$ cannot be derived, and so it will always be legal to make an assumption in at least one direction from the base world for a given fact.

More concretely, we now say a program $\Pi$ is a world iff:
% Here, P is a non-negated ground fact, and it's universally quantified
\begin{itemize}
	\item $\Pi \not \models \bot$
	\item If $\Pi \not \models P$ and $\Pi \not \models \neg P$, then $\Pi \leadsto \Pi'$ s.t. $P \in \Pi'_E$ or $\neg P \in \Pi'_E$
\end{itemize}

And we say $\Pi \leadsto \Pi'$ if
\begin{itemize}
	\item $\Pi'$ is a world
	\item $\Pi \not \models \bot$
	\item $\Pi \not \models P$
	\item $\Pi \not \models \neg P$
	\item One of:
	\begin{itemize}
		\item $\Pi'$ is $\Pi$ with $\neg P$ added
		\item Adding $\neg P$ does not yield a legal world, and $\Pi'$ is $\Pi$ with $P$ added.
	\end{itemize}
\end{itemize}

Reconstructing the $\leq$ relation inductively as above, we have acquired call/cc.
Effectively what we have done is allowed positive assumptions, but only in the case that negative assumptions fail.
We have also structurally ruled out those cases where we have made the truth or falsity of a fact inconsistent.
It would be possible to write similar rules without the forced negative bias (simply removing the restriction in the $\leadsto$ relation), and would possibly even be beneficial for a logic with guaranteed finite universes.
However, as we will see next, this enforced negative bias allows us to have this looser form of negation even in the presence of infinite universes.

We actually need even less modification than last time to reach actual circumscription.
In this case, we are still adding positive facts, but we need different restrictions since it is not as simple as $P$ versus $\neg P$.

We say a program $\Pi$ is a world iff:
\begin{itemize}
	\item $\Pi \not \models \bot$
	\item If for any $P_c$, $\vec{x}$, $\vec{b}$ with $\Pi \models P(\vec{x}, \vec{b})$, there does not exist a $\vec{a}$ so that $\Pi \models P_c(\vec{x}, \vec{a})$, then $\Pi \leadsto \Pi'$ s.t. for some $\vec{c}$ $P_c(\vec{x}, \vec{c}) \in \Pi'_E$ or $\exists \vec{d}. \Pi \not \models P(\vec{x}, \vec{d}) \& P(\vec{x}, \vec{d}) \in \Pi'_E$
\end{itemize}

And we say $\Pi \leadsto \Pi'$ if for some $\vec{x}$, there isn't a $\vec{a}$ so that
\begin{itemize}
	\item $\Pi'$ is a world
	\item $\Pi \not \models \bot$
	\item $\Pi \not \models P_c(\vec{x}, \vec{a})$
	\item One of:
	\begin{itemize}
		\item $\Pi'$ is $\Pi$ with $P_c(\vec{x}, \vec{a})$ added
		\item There isn't a legal world with $P_c(\vec{x}, \vec{a})$ added, but if we added it anyways, and deleted the rule for inconsistency on $P_c$ for $\vec{x}$, that world would have a final fact $Q(\vec{y})$, and $\Pi'$ is $\Pi$ extended with $Q(\vec{y})$
	\end{itemize}
\end{itemize}

\paragraph{Summary}
\todo{Design unified notation in summary, revised other parts to use unified notation}

Model Components:
\begin{itemize}
	\item Universe, $U$ (model native identifiers for values placed in fields)
	\item Interpretation, $I$, $I^{-1}$ (function mapping between original program and the universe)
	\item Predicates, $P$ (membership of $u \in U$ for $P$ denotes $P(I(u))$ in the output)
	\item Callbacks, $f_M$ (callbacks from the original program, translated by $I$)
	\item Meets, $\wedge$ (meets from the original program, translated by $I$)
	\item Aggregation Choices, $P_a$ (sets indicating which aggregations the model chose to reveal)
	\item Assumptions, $\Pi' - \Pi$ (Aggregated facts which are assumed to have no element above them)
\end{itemize}
Verification procedure:

\subsection{Search Procedure}
\label{sec:search}
Now that we've defined what the language should output, we present a search procedure for computing them which is implemented concretely as Mycroft (\S~\ref{sec:mycroft}).
\subsection{Progress}
\label{sec:progress}
Here, we define progress for the search strategy to mean that we will never repeat a state.
\subsection{Soundness}
\label{sec:soundness}
%TODO xref terminal worlds, make sure predefined
Soundness here means that if the search strategy terminates, we will output a set of facts consistent with the semantics of the program at one of the terminal worlds.
\subsection{Termination}
\label{sec:term} 
Our termination property refers to the notion that if the program has only finite sets in its correct outputs, we will eventually reach one of those sets.
Initially, this might seem to come freely from progress and soundness.
If we never repeat a state and the outputs (and so the number of sound facts) are finite, shouldn't we always terminate, since we are evolving through a finite set of states without repetition?
Unfortunately, this is not quite enough.
We also need to ensure that the search strategy also guards against infinite evolutions in worlds which are not present on the kripke structure (e.g. are for one reason or another not a valid world).
First, we will show, as in the supposition above, that we spend a finite amount of time progressing through worlds which are present on the kripke structure defined by the program.
Secondly, we will demonstrate that digressions from this tree are finite in length - that is, we will discover an invalid branch choice in finite time.
Finally, we will show that there are a finite number of ways to leave the kripke structure in the search procedure. 
If we spend finite time on kripke structure, make a finite number of departures for it, and each of those lasts for a finite amount of time, we will eventually terminate.

%TODO actual proof, this is just an outline above
\section{Implementation}
In order to evaluate the language as a means towards program analysis, we need a running implementation.
\subsection{Holmes (Old Implementation)}
Initially, I produced a database backed implementation which compiled down to a combination of Rust and SQL (initially C++ and SQL) and had Postgres handle the business of joins, deduplication, and data storage.
This had the advantage of being able to handle significantly larger working sets in theory, but in practice had significant performance issues which lead me to change approaches.
Despite this, I feel it is worth discussing here both because the failures of the implementation point out some of the unique challenges and simplifications that can be made in evaluating datalog, but also because it seems inevitable that to analyze programs substantially larger than those examined in this thesis, either a distributed platform or a disk-backed system will need to be used.
It is my hope that these lessons learned will help a future external-database based implementor avoid the same pitfalls.
Most of the details here are focused on Postgres, but other systems take a generally similar approach so similar problems are likely to occur.

As a result, this section is mostly focused on what went wrong, rather than on how the system was constructed.
If you want to see how the system was constructed, source is available at \url{https://github.com/maurer/holmes}, but be aware that it does not represent a complete implementation of the language.
In particular, it only has partial support for aggregation, and no support for circumscription.

\subsubsection{Indices}
% Which indexes to make?
%TODO cite postgres/mysql/mssql?
Database software usually does not know which indices would be ideal to keep, and since keeping extra indices is is expensive in both time and disk, most SQL systems require the user to specify the indices to keep manually.
Work is ongoing~\cite{peloton} to remedy this problem, but is not yet a production tool.
In the meantime, if we wish our translated datalog queries to run efficiently, the database must be provided with a list of indices to keep.

I tried a number of heuristics, including indexing in a global attribute ordering, indexing per query based on left-to-right joins, and just indexing all fields in order, and having the programmer reorder fields to boost performance.
None of these approaches worked in practice.
Both the global ordering and the left-to-right joins failed in large part because the query planner would choose to reorder the joins at runtime in multiple different ways.
The programmer manually ordering fields could find local optima, but because predicates are used in multiple ways, it too falls short.

The solution in use at the time this approach was switched away from was to annotate the program with an explicit set of indexes to keep.
I generated these indices by profiling the running program, and adding indices which would allow the query planner to avoid nested loops or full table scans where possible.

\subsubsection{Append-Only, High Write}
% Append only workload
One interesting aspect of a datalog system that the workload is entirely append-only other than retraction events, which are intended to be rare.
This knowledge is unused by the database in executing queries.
If it materializes a view to execute a query, and an underlying table is updated to by an append, it will re-materialize the whole view, not perform any kind of incremental maintenance.

One of the expensive parts of many queries was insisting that it only return results which contained at least one \emph{new} fact - one which hadn't been returned in this query before.
That tables can only be appended to could enable the incremental maintenance of the join, allowing more efficient computation of the join, and retrieval of only the new data.

There are also some database schemas (such as the star schema) which become more possible in the absence of mutation or deletion.
\subsubsection{Query Planning}
Query planning, while of benefit to users who do not know all their SQL ahead of time, or whose tables remain in steady states, was the biggest issue with this approach.
Databases commonly use a component called a query planner to translate SQL statements into an internal representation (loops, merge joins, hash joins, index walks, etc) that they can concretely execute.
This component depends on a variety of information, including but not limited to:
\begin{itemize}
	\item Whether the statement was prepared
	\item If prepared, how many times it has been executed
	\item What indices are available
	\item Information from the statistics daemon
\end{itemize}
Other than examinining what indices are available, these conditions turn out to be highly anti-productive for a datalog workload.

The statistics daemon is designed with the assumption that there is a sort of ``steady state'' for a database, in which the relative sizes of the tables will remain similar.
This makes sense for usual customers of databases, but in our case, a large part of operation looks like heavy insert activity on a specific table.
As a result, the statistics daemon's information is generally woefully out of date.

We prepare virtually all statements, since we intend to execute them repeatedly and want to avoid time in the parser.
However, as of the time this system was developed, postgres would ossify the query plan as of the 5th time a prepared statement was executed.
This was done based on the assumptions that SQL connections do not live so long that the database changes a lot, so by the fifth time the query is run, the plan is unlikely to be improved, and performance will be increased by avoiding the planner entirely.
In practice, this means that any recursive rule (like one marking nodes as reachable, or performing a dataflow) will run terribly.
The rule executes five times, and during that time, the statistics daemon either has old out of date information, or even if it updates, information that the table it's reading out of is terribly small.
The query planner then makes bad decisions based on this, then sets them in stone.
As a result, indices sit there unused, and logarithmic operations are done linearly.

If the statements are not prepared, we incur parsing and planning overhead on every query.
While unfortunate, those costs were low in comparison to the troublesome queries.
The true problem with completely non-prepared statements is that the query planner would rapidly change strategies, meaning that which indices are needed would change at different points in execution.

Since in our case we have a fixed query set and a rapidly changing database, it would most likely make more sense to absorb the query planner into the compilation process somehow.
Postgres did not at the time of implementation have a way for a client to provide it with an explicit query plan short of building and providing a plugin which ran said plan as a function.

\subsubsection{Star Schema}
%TODO: note that this is equivalent to picking a good $I$ for our model, finding it, then swapping it back to Herbrand at the end
As alluded to earlier, one benefit of an append-only workload is that star schemas are massively more useful.
Star schemas are normally used for ``data warehousing'', a sort of large scale database where an organization's data is all loaded into a single schema before being pulled out again into smaller databases for actual processing.
The idea is that most values are referenced rather than included directly in tables.
Warehousing personell are largely interested in the standardization of these values and the resulting compression.

In our case, a star schema is interesting both for reasons of compression, and for ease of indexing.
Indexing an IL instruction, whether by hash or by ordering, is much slower than sorting by a tuple of integers.
I discovered this technique after the pivot to an in-memory database, so I have no observations of its performance, but I expect it would help.

\subsubsection{Large Objects}
With an external database, the use of large objects becomes nontrivially expensive.
If the database is local, and the bus between the program and the database shared memory, this is not a major issue.
However, even over a local unix socket, repeated accesses to large objects can inhibit performance.

This shows up in practice when dealing with binary sections and segments during lifting.
If the lifting rule needs the segment, the architecture, and an offset into that segment to perform the lifting, this can incur several copies of the segment per instruction.
In my sample programs, most segments were between 300k and 600k, causing this to incur a nontrivial cost.

The first solution, specific to this problem, was an all-at-once chunking of the segment.
I requested the segment from the database, then produced a 16-byte chunk (maximum length of an x86 instruction is 15 bytes) at every offset, and sent it back.
In the future, requests would access this chunked data rather than the original.
This resulted in a 16-17x blowup of the space to store the base binary, but as that paled in comparison to everything else it was not especially significant.

The second solution was to add another extension to datalog allowing some functions to exist as special external predicates to be run database side.
These all needed to be builtins, and while the approach was slightly more efficient, overall I no longer think the improvement warranted the complexity.

If I were to address this again today, I would use a star schema database side, and implement a cache client side for fetched star objects.

\subsection{Mycroft}
\label{sec:mycroft}
Mycroft is a row-oriented, single-threaded, in-memory datalog engine, taking into account the experiences of the initial implementation.
It operates as a macro which transforms datalog into Rust code, which can then be compiled into a running program.
In its current form, it addresses most, though not all, of the pain points encountered with Postgres.
The query planner is replaced by a single plan, generated at compile time, which parameterizes itself only on the size of the relevant tables at that moment.
This replacement also means that we know precisely what indices will be useful, and can generate them.
The join algorithm is aware of the incrementality of append only joins, and uses this to speed up requests for new results. 
As mycroft is in process and in memory, large objects are not a problem.
They are returned as read-only references to the existing strucure, and can be operated on that way.
The implementation is available at \url{https://github.com/maurer/mycroft}, and as a crate on \texttt{crates.io} for direct inclusion in rust projects.

\subsubsection{Typed Storage}
Rather than store data values directly in rows as was done in the Postgresql-based implementation, here we keep a separate deduplication table for each type of data on which we operate.
This allows us to efficiently map back and forth between values, which the callbacks need to consume, and integer keys, which are convenient for or indexing and join algorithms. 
This is reminiscent of the star schemas discussed before.
As our system is mostly-append (other than retractions due to circumscription) we design this as an insert-only structure.
An additional benefit, more relevant here than with Postgres, is that this greatly reduces our memory footprint.

At compile time, each type present in one or more predicates has a modified robin-hood hash table declared for it.
This table has two pieces: a vector backing which stores the actual data, and a vector of hash/key pairs.
There are two operations this table needs to support: acquiring the key for an object, whether or not it's present already, and acquiring an object from its key.
Finding the key for an object is accomplished by using a lookup on the hashtable portion of the structure, inserting into both the table and the vector of data in the event of a lookup failure.
Finding the object for a key (the more common case) is works by indexing into the vector.

The only principal difference between this and a simpler design (a standard hash table mapping from the value to the key, and a vector mapping from a key to a value) is that it stores the data only once, and without any indirection.
While this may not sound like much, this gave a modest 23\% time performance boost over the standard library implementation in time, and approximately halved space on an earlier version of the use-after-free detector.
The closest approach still using the standard data structure would have been to use a smart pointer to share data between the data structures, or a hashtable of hashes.
The smart pointer caused trouble with the interfaces, and hashing twice incurred a performance penalty, so we used this solution.

\subsubsection{Aggregation}
As aggregation is described at the predicate level, we can implement it directly on the tuple storage.
Tuple storage is structured as a map from the tuple of non-aggregate fields (reordered to the front) to a tuple of aggregate fields.
These aggregate fields are represented by a triple of the value-keys to be aggregated, a current aggregate value, and an index indicating how many of the value-keys are aggregated in the cached value.
This allows for a lazily updated computation of the meet.

When a tuple is inserted into the store, if a value with the same non-aggregate fields is present, in which case the value-key list is extended, but the aggregate is left alone.
If it is not present, we initialize the aggregate value with the value of the key in that slot, fill in the key in the keys-to-be-aggregated, and set the index to 1.
When retrieving a tuple, we check whether the index is equal to the length of the comprising keys.
If it is not, we start the iteration at the index, and perform iterative meets until the aggregate is up to date.
We then return the tuple, extended by the aggregate fields and reordered.

\subsubsection{Join Computation}
Datalog computation is join-heavy, and as a result attempting to compute the join naively can lead to disasterous execution times.
There are a variety of existing join approaches.
% Cite postgres?
RDBMSes tend to favor straightforwards strategies, such as nested looping, hash join, and merge join.
Merge joins require a relevant index, but generally perform substantially better unless tables are extremely small.
Hash joins operate by creating an intermediate data structure of one of the tables which is indexed by the hash of the joined values.

However, for high-arity join patterns, better algorithms exist, usually formulated as ``worst-case join'' algorithms.
Ngo showed~\cite{nprr} that it is possible to develop join algorithms which are optimal even under these conditions.
This algorithm is rather complex, and is intended for theoretical results rather than actual implementation.
However, LogicBlox~\cite{logicblox} developed an algorithm known as Leapfrog Triejoin~\cite{lftj} which achieves these same bounds while remaining practically implementable over traditional indices.
Unfortunately, this algorithm is patented, and so could not be used.
This indicates a potential for future implementations to derive a novel approach from the AGM~\cite{agm} bound or Ngo's~\cite{nprr} approach, but developing such an algorithm is beyond the scope of this thesis.

In mycroft, we used a simultaneous merge join ordered from smallest table to largest table.
An index is selected for each table which walks it in unification argument order, with constant arguments being sorted to the front.
The first index is advanced to the first tuple where all the constant arguments match.
This is made easier by the use of integer-only tuples, as the non-constant arguments can be represented as 0 in a query to the index.
Then, candidate variable bindings are made to the unification terms (if possible) and the next table is considered.
When on the last table, if a candidate set of assignments to the unification terms can be completed, it is emitted and the index advanced by one step.
If the index cannot advance or the index fails to unify with earlier tables, we know that no further result is possible, and go back one table, and continue.
This approach keeps around only a small amount of additional state, linear in the number of clauses in the query, as it is returning results.

However, due to our need for \emph{incremental} results, we can improve this mechanism substantially.
Rather than computing the entire join at once, we split it into subjoins, one for each clause in the query.
We have a separate, much smaller index for ``new'' facts in referenced predicates, requested by the query at database initialization time.
We perform a subjoin with each predicate's large index swapped for this small index to get exactly those results which we would receive that we did not before, then chain them for a result.
The small indexes are emptied during this operation, so they will not yield the same results again.

As an example, consider evaluating the query $A(x, y) \& B(y, z) \& C(z, x)$ for incremental results.
The first time it is evaluated, we perform a full join, ignoring the subjoin strategy - it would be equivalent to performing the full join 3 times.
Then, we insert two facts into $A$, and one into $C$.
Running the query again, we perform three subjoins, one on $A', B, C$, one on $A, B', C$, and one on $A, B, C'$.
In our join algorithm above, remember that we sorted the smallest table to the left.
As a result, the join with $B'$ immediately terminates, yielding no results.
For the join with $C'$, it essentially acts as a join on $A$ and $B$ only, with a constant restriction.
The join with $A'$ is similar, but the $A'$ portion of the join yields two facts, so it essentialy runs two constant constrained joins of $B$ and $C$.

\subsubsection{Provenance Tracking}
In order to later manage circumscription, or to allow a human to trace the reasoning of a program, we need to keep track of where facts come from.
To do this, in conjunction with each tuple we store a list of possible justifications.
A justification is composed of the ID of a rule, and the IDs of the facts used to match the body clause of that rule.
An aggregation is represented simply as the list of fact IDs aggregated for the match.
A map is additionally maintained from fact IDs to justifications which contain them.

\subsubsection{Circumscription, Call/CC, and Retraction}
Implementing circumscription essentially involves monitoring accessed aggregations to see if they would change, and responding with a retraction.
The previous description of aggregates does not easily allow for this.
A tuple insertion does not know if something has depended on this aggregation's completeness, and if so what.
To deal with this, if a tuple is circumscriptively fetched, we replace list of merged keys in the aggregate field with a newly minted aggregate ID.
Three maps are maintained for aggregate IDs:
\begin{itemize}
	\item Aggregate ID to comprising Fact IDs
	\item Aggregate ID to dependent justifications
	\item Fact IDs to Aggregate IDs they comprise
\end{itemize}

If a tuple insertion occurs and would need to update an aggregate represented by an aggregate ID, that aggregate ID is retracted.
The retraction code acts as a worklist, initially populated by the broken aggregation.
First, it removes any justifications broken by the current retracted item.
Then, it retracts (by adding to the worklist) any facts which now lack justification.
If the current retracted item is a fact, it also retracts any aggregate IDs which now have one fewer fact.

In the special case where the tuple just inserted was also retracted, we replace its justification with one referencing the members of its now broken aggregate ID.
This ensures that while this justification no longer cares about the expansion of the circumscription, it will still be properly retracted if one of the facts in the original aggregate becomes invalid.

%TODO explain how we deal with cyclically supported facts
% e.g. \neg B -> A; A -> A; discover B, how do we ensure we retract A?
% Also explain how this works in the presence of circumscription.

\subsubsection{Future Work}
There is plenty of room for improvement in the concrete implementation of the language engine.

Currently, we keep more indices than are strictly necessary.
Even with our current join strategy, the count of indices kept scould be reduced through a mechanism to match attribute ordering between queries more frequently.
With a more modern join like tetris join~\cite{tetris} it could even be possible to keep a single index per predicate.

Results of some subjoins get used repeatedly, and can be known not to change through topological sorting.
Currently, this is exploited through manual tabling - the creation of dummy predicates to keep the completed join as a realized structure.
However, it should be generate these temporary structures automatically in some cases.

Pivoting indices from a simple in-memory btree to a MVCC-style structure would allow multiple worker threads to be evaluating rules at the same time.
As modern systems generally have additional cores, this should lead to performance improvements overall (though degradation in bottlneck phases).
This approach also meshes well with optionally backing some data structures with disk due to either large size or low traffic.
Many MVCC trees are already designed as on-disk data structures due to their use in traditional RDBMS systems.
Allowing some data to reside on disk would increase the maximum size of analysis the system could perform on a single binary, or allow for easier multi-binary analysis.

In an ideal world, this system could even be distributed.
Other than circumscription and the decision to terminate, every component of this system can operate safely with a partial knowledge of the database.
As a result, it seems plausible that with appropriate heuristics for shuffling and synchronization around circumscription, this language could be well suited for distributed execution.
\section{Benchmarks}
