\chapter{Holmes}
% We've argued why datalog, now argue why we want these extra features
Holmes is a dialect of datalog, tailored with extensions for the specific use case of analyzing compiled code.
Specifically, a normal dialect of datalog will fall short on several tools desired by the analysis author:
\begin{itemize}
		%TODO expand list
	\item Data structures
	\item Aggregation
	\item Negation
\end{itemize}

\section{Feature Selection}
To address these lacks, we add a few features to base Datalog.
%TODO once expanded list is there, do short "X address Y and Z, Q addresses..." blurb
\subsection{Callbacks}
%TODO jam widening in dataflow somewhere in here as a justification?
% Why do we need External Predicates
%TODO rewrite condition - "computation" too vague
Tasks which do not involve a fixpoint, but do involve computation, can frequently be both more difficult and more expensive to write in pure datalog.
For example, parsing an ELF and splitting it up into segments, sections, generating records for its symbols, etc. could in principle be written in datalog.
However, this would be difficult to write (operating on a string as a linked list, or similar structure), slow to compute due to high arity joins, and would require that the input first be transformed before even entering the program.
Other similar examples include lifting (translating a sequence of bytes into a semantics IL), concrete execution, and arithmetic.

Previous approaches have noted that many of these steps come towards the beginning of analysis, and perform these tasks as a precompute phase before handing the results to datalog to process.
In our case, we are trying specifically to avoid such phasing.
The lifter might be needed again for previously undiscovered code.
The loader might be needed again if we discover a call to \texttt{execve} and wish to follow it.
Doing a phased datalog prevents the easy interleaving of these functionalities into the global fixpoint.

While datalog predicates can be a great data structure, they are not always the best data structure for all tasks.
Datalog predicates can effectively be viewed as an append-only, deduplicated, index-by-anything table.
While this is a great data structure for many tasks, some concepts are better represented in other ways.
One example is ILs and ASTs.
As frequently nested, branched structures, they \emph{can} be represented in datalog, but walking one would take a very large number of lookups compared to using a traditional ADT approach, not to mention the clumsiness.
Other similar concepts include formulae (as in SMT) and any kind of variable-length buffer representation.
All of these can be done in pure datalog, assuming appropriate preprocessing has been done.
However, the resulting time and space costs make this something to be avoided.

To address the above two, we add the ability to register a callback to a datalog rule.
If specified, whenever a that rule fires, the corresponding callback will be used supplementarily to determine the values to substitute into the head.
This allows use of traditional functional or imperative style code to implement data structure transformations or perform operations which would be slow to do in the base datalog.
Additionally, it allows us to more readily incorporate existing code (such as the BAP~\cite{bap} lifter) rather than rewriting it from scratch.

This is equivalently powerful to external predicates in other languages in terms of expressivity.
Any callback specified could instead be turned into an external predicate and simply appended to the query.
A query involving external predicates might need to be split up into phases to be expressed in callbacks.
If an output variable of an external predicate is present in another term in the query, one would need to use intermediate tables, as the callback only occurs at the \emph{end} of a query, and there can only be one per query.
This simplifies the design of the datalog engine (the join engine is entirely separate from the callbacks), at the cost of the ability for a sufficiently advanced engine to better optimize such queries.

\subsection{Monotonic Aggregation}
Traditionally in datalog, your match clause may only access a fixed (though arbitrary) number of facts at the same time.
Even counting can be difficult.
To verify that there are at least three instances of some predicate p, you would normally write:
\begin{verbatim}
p(x) & p(y) & p(z) & neq(x, y) & neq(y, z) & neq(x, y)
\end{verbatim}
The size of this query grows as $n^2$ in the number of elements you are counting.

This same difficulty occurs when encoding a dataflow or abstract interpretation algorithm into datalog.
When two branches come together, a new fact representing the state with the meet applied needs to be generated.
If we do this naively, simply matching on the existence of two states at that program point and generating a new one by merging, we will again go rapidly superlinear.

In existing systems~\cite{doop1} this is dealt with by ensuring the state in question can be extended simply by adding more facts.
When possible, this works nicely, but it prevents the use of things like strided intervals~\cite{vsa} or widening operators in dataflow algorithms which lack finite descending chains.
This is because all of these situations require reasoning about a variable sized subset of the data to make their conclusion, not just a fixed window.

Finally, in the case of employing an external solver, they often need to receive all the inputs up front, rather than incrementally.
Calling out to an SMT solver will not work if your formula is stored as facts in a datalog representation; you would first have to walk them with a rule and a callback (or a rule and an external predicate in another system) to build up a viable representation and hand it off.
The same is true even of simpler concepts, like applying Steensgaard's algorithm~\cite{steensgaard-alias} to a set of constraints - it will need all of them, or you will end up storing incremental program states in your database as well, and end up back at the $n^2$ problem.

Traditionally, this is dealt with by appling a postprocessing step to the datalog computation.
After it is done, a query is run, and the aggregation is performed by an outside-of-datalog program.
As stated earlier though, we want all portions of the analysis to be able to trigger all others to avoid explicit phasing.

Some of these specific scenarios can be worked around with tricks, but they do not apply universally and are still awkward to write.
For example, a more clever author of the counting check might instead use a greater than operator instead of not-equals, assuming that field is ordered.
The resulting query would then only be linear in the count you wanted to check against.
However, it would still not be possible to have a variable limit to check against without significant complexity.

To address these issues, we allow for predicates with aggregation.
If a predicate is declared with aggregation, a provided meet operator will be used to aggregate submissions to each aggregate field for which all non-aggregate fields match.
This addition allows us to address each of these issues.
In the case of counting, we simply use set-union as our meet operator.
For dataflow or abstract interpretations, we can have parameters like program location be non-aggregate fields, while the state is an aggregated one.
Programs using this feature need to be aware that they may receive the aggregation of any subset of derived facts, and are only gauranteed to ever receive the aggregation at the fixpoint.
%TODO add forwardref here for why

\subsection{Hypothetical Circumscription}
Some questions revolve more around what isn't there than what is.
For instance, if \texttt{ud2}\footnote{
LLVM inserts this instruction to denote unreachable code, and is intended to cause a trap if hit
} is found in the binary, we might wish to determine if it is in fact statically unreachable.
This requires us to be able to state that we know \emph{all} of the edges entering that basic block, not some subset.

As a more concrete application, if we have an algorithm which works on the SSA representation of a function, creating an SSA representation of that function requires the entire control flow graph.
If we add edges later, conclusions derived from the incomplete SSA form might become incorrect.

Traditional datalog either disallows negation, or allows it through explicit stratification.
In stratified negation, each predicate belongs to a ``strata''.
The database is computed by sequentially adding the rules that have heads in each of the new strata in sequence.
Since all the rules which could generate a particular predicate will not be used again, we can assume (using negation-as-failure) the negation of any fact not yet in the database at that point.

Not all programs are written such that stratification is possible.
Consider a graph with vertices for each predicate, and a edges for a predicate which may be derived from another predicate.
Mark any edges for which the body-side predicate was negated as negated.
If this graph has no cycles which include negation, then the program can be stratified.
Contract strongly connected components on this graph to a single vertex, and then output in topographically sorted order to generate a stratification.

In the context of doing program analysis on binaries, we might wish to avoid this even when reasoning purely monotonically.
Consider an analysis which determines whether a function will never return.
This information is important in analysis of a calling function because it should not expect control to proceed past the called function.
To declare that a function will never return when called, we must know \emph{all} the paths within it, not just some of them.
As a result, we are implicitly talking about knowing the negation of additional edges in the control flow graph.
If we employed stratified negation, we would have to close over the entire control flow graph at once, leaving us unable to employ this information in the CFG definition for a calling function.

To address this need, we add hypothetical circumscription.
The core concept is that we can at need assume that a particular chunk of information is expanded, and reason forwards.
In the event that this turns out to be false, we can retract that assumption, and reason forwards again.
This allows us to deal with cases of negation which are not trivialy stratified.
In the language, this feature is implemented as the ability to match on an aggregation and know your rule has received only the aggregation which will be present in the final fixpoint.
Contrast that with the monotonic aggregation, where your rule must have correct operation for any subset of the possible aggregations along the way.
This can be used to implement the stratified case in a straightforwards manner, and also to support dynamic negation as describe in the never-returning example above.

\subsection{Call/CC}
The astute reader may have become worried at the end of that last subsection, as it is not without reason that the standard approach to negation involves stratification.
In the case that we have a negated inference cycle not just on predicates, as stratification prohibits, but on the actual facts, the approach as described so far would lead to alternation and indeterminacy.

Interestingly, there is actually a use case for allowing negated cycles in program analysis.
In the case of the outputs of an dataflow analysis or a control flow recovery, we will need to circumscribe over their results in order to know we have received the actual output.
Using an incomplete run of an alias analysis, for example, would result in too-small upper bounds being used.
However, due to the interconnectedness of these examples, the complete alias analysis information might alter the control flow graph by refining the information used to determine the target of indirect jumps.
Changing the control flow graph would in turn invalidate the circumscribed alias analysis.

Strictly looking at the system thus far, this would loop.
Altering the control flow graph would retract the alias analysis, which would retract the alteration to the control flow graph.
Thinking about what a human would do if they had gone down the same reasoning path points to a potential solution.
If you had assumed you saw the totality of control flow, and from that, came to find a new control flow edge, you would assume that your initial assumption must have been wrong, and that edge really is there.
Essentially, you would assume $(\neg P \rightarrow P) \rightarrow P$.

This matches the type signature of \texttt{call/cc}, and not without reason.
In this case, the continuation is the reasoning strategy forwards, assuming P can be determined to be true.
If P is not determinable to be true, this continuation cannot actually be invoked, and we never go down that path.
If P can be determined to be true, in a traditional programming language we might go down that path.
In our case, we are constantly watching for P to be determined to be true, and if it is, we immediately take the continuation.

This feature is invisible to the user other than for performance characteristics.
The user need only specify their rules as usual, circumscribing over things which need to be complete, and if this \texttt{call/cc} condition arises, it will be automatically dealt with by reasoning forwards from the new (expanded) circumscription, after retracting relevant other derivations.

\section{Informal Semantics}
Before giving a more formal treatment of what outputs are correct for a given Holmes program, we describe how each feature functions informally, as you might see in a programmer's guide.
Afterwards, there is a simple traced execution of a toy program which employs all of these features.

\subsection{Callbacks}
External predicates are traditionally expressed as predicates in the language, equipped with modes for each field and a function which transforms from the input fields to some representation of the output fields.
This model can add some complexity to both the inference engine and writing code in the language.
If an external predicate is present in the match, how should it be searched?
There are several potential strategies, and complexity only increases when there are two external predicates present.
From the programmer's point of view, the monodirectionality of the predicate is concealed, which can lead to surprise when a predicate can't be used the same way as others.

To address this, I explicitly phase the application of rules.
First, there is an initial match phase that works as usual.
Then, optionally, the rule may have a function which takes as input a set of assignments to variables in the match clause, and gives back a list of assignments to any variables present in the head which were not defined in the match.
This list may be empty, and this result indicates match failure.

For example, in the rule
\begin{verbatim}
simple_func: p(y) <- q(x) & r(x) + func
\end{verbatim}
\texttt{func} would be expected to take in the value of x, and return a list of values for y.

\subsection{Monotonic Aggregation}
Monotonic aggregation is defined as a property of a subset of predicate fields.
A result is legal in a match against an aggregated predicate if there exists a specific subset of derivable facts which match via equality on all non-aggregation fields, and for whom applying the aggregation function provided to the remaining fields would produce the result.
A result is mandatory if the selected subset is the largest possible for some indices.
For a predicate \texttt{p(i32, i32, IntSet\^union, i64\^max)}, an attempt to match against \texttt{p(a, b, c, d)} with a database \texttt{p(1, 2, {3}, 4), p(1, 2, {4}, 3), p(1, 1, {}, 7)} would be \texttt{p(1, 2, {3, 4}, 4), p(1, 1, {}, 7)}.

%TODO finish section
%TODO: mention reason for not using a lattice initialism is that there would be a potentially infinite number of matches that produce it, so matching against a predicate with aggregation whose indices were not fully constrained by the rest of the match clause would produce an infinite number of results in the case of non-finite domains
\subsection{Hypothetical Circumscription}
Hypothetical circumscription extends monotonic aggregation by allowing the rule to receive only the largest subset.

This is a form of negation.
To illustrate, consider the program
\begin{verbatim}
p(unit)
neg_p(unit)
q(bool^and)

q_init: q(true) <-
pq: q(false) <- p(())
neg: neg_p(()) <- ~q(true)
\end{verbatim}

The predicate \texttt{neg_p} now contains the negation of \texttt{p}.
This formulation can be extended in a straightforwards way for predicates whose indices have finite domain:

\begin{verbatim}
p(finite_type)
neg_p(finite_type)
q(finite_type, bool^and)

q_init_i: q(x_i, true) <- 
pq: q(x, false) <- p(x)
neg: neg_p(x) <- ~q(x, true)
\end{verbatim}
where \texttt{q_init_i} is reproduced for every $x_i$ in \texttt{finite_type}

The intuitive reason for this connection is that knowing the largest possible aggregation also entails the knowledge that all of the other possible members of the aggregation cannot be derived.
In the case of finite domain indices, this allows us to negate the predicate in the way described above, since we can subtract the aggregation returned form the universal aggregation.

When the domain of an index is infinite, we could still construct the negation through use of an external function capable of enumerating the index values, e.g.
\begin{verbatim}
q_init: q(x_0, true) <-
q_step: q(x', true) <- q(x, true) + succ
\end{verbatim}
however, the resulting database would be infinite, so this would really only have solid meaning under a minimal model interpretation.

Since I am adding a construct with the power of negation, a natural question is how I will deal with inconsistency.
The traditional approach here is to stage computation, requiring a predicate to saturate before its negation can be considered.
For the call/cc feature next, and to deal with new facts (e.g. from a concrete program execution) added to the database after inference has begun, I can't actually use this stratification.

In an intuitive form, my solution here is to consider all negation as hypothetical, and consider answers from as far along a consistent hypothetical tree as possible.

More formally, I describe this relation using kripke semantics.
Initially, I will deal only with finite-domain indices.
When dealing with finite domain indices, we can rewrite (however inefficiently) every rule requiring circumscription to one making use of negation.
Specifically, for each rule containing a circumscription in the match, for all aggregated variables, for all values in their domain, write the rule with the variable substituted for the value and appended with the negation of all other values.
%TODO proof sketch of the equivalence.
Chaining this with the rewriting of aggregation into a less efficient form without aggregation, and using the finite domain property to reduce external functions to tables, I can reason about datalog with negation here.
Specifically, assuming a finite domain for all index variables, all extensions described so far can be reduced to basic datalog with the added ability to negate clauses in the body of a rule.
Note that negated heads are not required here, and so are not under consideration.

Define a candidate world $\omega$ to be a tuple of a context $\Gamma$ containing program rules and some set of negated facts $N$.
Define $\omega \models P$ to mean that $P$ is in the minimal model of $\Gamma$, interpreting the matching of negated facts to only be able to match those facts explicitly in $N$.
Define the candidate accessibility relation $\omega \cac \omega'$ to mean that $\omega'$ is $\omega$ with its $N$ augmented with a $P$ not in $N$, which is present in the right hand side of some rule in $\Gamma$.

Define a world to be a candidate world in which $\omega \models P \imp \omega \not \models \neg P$, $\models$ to be as in the candidate case, and the accessibility relation $\leq$ to be the subset of $\cac$ which is only between worlds at this level, augmented with reflexivity.
Additionally, define $w \vdash P$ to mean either $w \models P$, or in the case of $P = A \rightarrow B$, $\forall w'. w \leq w', w' \vdash A \imp w' \vdash B$.
This forms an intuitionistic Kripke frame.
By definition of the upgrade from candidate world to world, $w \not \models \bot$.
By construction of $\cac$, the accessibility relation leads only to worlds where more things are in the context, and since the only non-monotonic operation in the logic is circumscription, which is excluded at the $\models$ layer, $(w \leq w') \imp (w \models P \imp w' \models P)$.

\subsection{call/cc}
\section{Formal Semantics}
\subsection{Progress Theorem}
\section{Implementation}
In order to evaluate the language as a means towards program analysis, we need a running implementation.
\subsection{Holmes (Old Implementation)}
Initially, I produced a database backed implementation which compiled down to a combination of Rust and SQL (initially C++ and SQL) and had Postgres handle the business of joins, deduplication, and data storage.
This had the advantage of being able to handle significantly larger working sets in theory, but in practice had significant performance issues which lead me to change approaches.
Despite this, I feel it is worth discussing here both because the failures of the implementation point out some of the unique challenges and simplifications that can be made in evaluating datalog, but also because it seems inevitable that to analyze programs substantially larger than those examined in this thesis, either a distributed platform or a disk-backed system will need to be used.
It is my hope that these lessons learned will help a future external-database based implementor avoid the same pitfalls.
Most of the details here are focused on Postgres, but other systems take a generally similar approach so similar problems are likely to occur.

As a result, this section is mostly focused on what went wrong, rather than on how the system was constructed.
If you want to see how the system was constructed, source is available at \url{https://github.com/maurer/holmes}, but be aware that it does not represent a complete implementation of the language.
In particular, it only has partial support for aggregation, and no support for circumscription.

\subsubsection{Indices}
% Which indexes to make?
%TODO cite postgres/mysql/mssql?
Database software usually does not know which indices would be ideal to keep, and since keeping extra indices is is expensive in both time and disk, most SQL systems require the user to specify the indices to keep manually.
Work is ongoing~\cite{peloton} to remedy this problem, but is not yet a production tool.
In the meantime, if we wish our translated datalog queries to run efficiently, the database must be provided with a list of indices to keep.

I tried a number of heuristics, including indexing in a global attribute ordering, indexing per query based on left-to-right joins, and just indexing all fields in order, and having the programmer reorder fields to boost performance.
None of these approaches worked in practice.
Both the global ordering and the left-to-right joins failed in large part because the query planner would choose to reorder the joins at runtime in multiple different ways.
The programmer manually ordering fields could find local optima, but because predicates are used in multiple ways, it too falls short.

The solution in use at the time this approach was switched away from was to annotate the program with an explicit set of indexes to keep.
I generated these indices by profiling the running program, and adding indices which would allow the query planner to avoid nested loops or full table scans where possible.

\subsubsection{Append-Only, High Write}
% Append only workload
One interesting aspect of a datalog system that the workload is entirely append-only other than retraction events, which are intended to be rare.
This knowledge is unused by the database in executing queries.
If it materializes a view to execute a query, and an underlying table is updated to by an append, it will re-materialize the whole view, not perform any kind of incremental maintenance.

One of the expensive parts of many queries was insisting that it only return results which contained at least one \emph{new} fact - one which hadn't been returned in this query before.
That tables can only be appended to could enable the incremental maintenance of the join, allowing more efficient computation of the join, and retrieval of only the new data.

There are also some database schemas (such as the star schema) which become more possible in the absence of mutation or deletion.
\subsubsection{Query Planning}
Query planning, while of benefit to users who do not know all their SQL ahead of time, or whose tables remain in steady states, was the biggest issue with this approach.
Databases commonly use a component called a query planner to translate SQL statements into an internal representation (loops, merge joins, hash joins, index walks, etc) that they can concretely execute.
This component depends on a variety of information, including but not limited to:
\begin{itemize}
	\item Whether the statement was prepared
	\item If prepared, how many times it has been executed
	\item What indices are available
	\item Information from the statistics daemon
\end{itemize}
Other than examinining what indices are available, these conditions turn out to be highly anti-productive for a datalog workload.

The statistics daemon is designed with the assumption that there is a sort of ``steady state'' for a database, in which the relative sizes of the tables will remain similar.
This makes sense for usual customers of databases, but in our case, a large part of operation looks like heavy insert activity on a specific table.
As a result, the statistics daemon's information is generally woefully out of date.

We prepare virtually all statements, since we intend to execute them repeatedly and want to avoid time in the parser.
However, as of the time this system was developed, postgres would ossify the query plan as of the 5th time a prepared statement was executed.
This was done based on the assumptions that SQL connections do not live so long that the database changes a lot, so by the fifth time the query is run, the plan is unlikely to be improved, and performance will be increased by avoiding the planner entirely.
In practice, this means that any recursive rule (like one marking nodes as reachable, or performing a dataflow) will run terribly.
The rule executes five times, and during that time, the statistics daemon either has old out of date information, or even if it updates, information that the table it's reading out of is terribly small.
The query planner then makes bad decisions based on this, then sets them in stone.
As a result, indices sit there unused, and logarithmic operations are done linearly.

If the statements are not prepared, we incur parsing and planning overhead on every query.
While unfortunate, those costs were low in comparison to the troublesome queries.
The true problem with completely non-prepared statements is that the query planner would rapidly change strategies, meaning that which indices are needed would change at different points in execution.

Since in our case we have a fixed query set and a rapidly changing database, it would most likely make more sense to absorb the query planner into the compilation process somehow.
Postgres did not at the time of implementation have a way for a client to provide it with an explicit query plan short of building and providing a plugin which ran said plan as a function.

\subsubsection{Star Schema}
As alluded to earlier, one benefit of an append-only workload is that star schemas are massively more useful.
Star schemas are normally used for ``data warehousing'', a sort of large scale database where an organization's data is all loaded into a single schema before being pulled out again into smaller databases for actual processing.
The idea is that most values are referenced rather than included directly in tables.
Warehousing personell are largely interested in the standardization of these values and the resulting compression.

In our case, a star schema is interesting both for reasons of compression, and for ease of indexing.
Indexing an IL instruction, whether by hash or by ordering, is much slower than sorting by a tuple of integers.
I discovered this technique after the pivot to an in-memory database, so I have no observations of its performance, but I expect it would help.

\subsubsection{Large Objects}
With an external database, the use of large objects becomes nontrivially expensive.
If the database is local, and the bus between the program and the database shared memory, this is not a major issue.
However, even over a local unix socket, repeated accesses to large objects can inhibit performance.

This shows up in practice when dealing with binary sections and segments during lifting.
If the lifting rule needs the segment, the architecture, and an offset into that segment to perform the lifting, this can incur several copies of the segment per instruction.
In my sample programs, most segments were between 300k and 600k, causing this to incur a nontrivial cost.

The first solution, specific to this problem, was an all-at-once chunking of the segment.
I requested the segment from the database, then produced a 16-byte chunk (maximum length of an x86 instruction is 15 bytes) at every offset, and sent it back.
In the future, requests would access this chunked data rather than the original.
This resulted in a 16-17x blowup of the space to store the base binary, but as that paled in comparison to everything else it was not especially significant.

The second solution was to add another extension to datalog allowing some functions to exist as special external predicates to be run database side.
These all needed to be builtins, and while the approach was slightly more efficient, overall I no longer think the improvement warranted the complexity.

If I were to address this again today, I would use a star schema database side, and implement a cache client side for fetched star objects.

\subsection{Mycroft}
Mycroft is a row-oriented, single-threaded, in-memory datalog engine, taking into account the experiences of the initial implementation.
It operates as a macro which transforms datalog into Rust code, which can then be compiled into a running program.
In its current form, it addresses most, though not all, of the pain points encountered with Postgres.
The query planner is replaced by a single plan, generated at compile time, which parameterizes itself only on the size of the relevant tables at that moment.
This replacement also means that we know precisely what indices will be useful, and can generate them.
The join algorithm is aware of the incrementality of append only joins, and uses this to speed up requests for new results. 
As mycroft is in process and in memory, large objects are not a problem.
They are returned as read-only references to the existing strucure, and can be operated on that way.
The implementation is available at \url{https://github.com/maurer/mycroft}, and as a crate on \texttt{crates.io} for direct inclusion in rust projects.

\subsubsection{Typed Storage}
Rather than store data values directly in rows as was done in the Postgresql-based implementation, here we keep a separate deduplication table for each type of data on which we operate.
This allows us to efficiently map back and forth between values, which the callbacks need to consume, and integer keys, which are convenient for or indexing and join algorithms. 
This is reminiscent of the star schemas discussed before.
As our system is mostly-append (other than retractions due to circumscription) we design this as an insert-only structure.
An additional benefit, more relevant here than with Postgres, is that this greatly reduces our memory footprint.

At compile time, each type present in one or more predicates has a modified robin-hood hash table declared for it.
This table has two pieces: a vector backing which stores the actual data, and a vector of hash/key pairs.
There are two operations this table needs to support: acquiring the key for an object, whether or not it's present already, and acquiring an object from its key.
Finding the key for an object is accomplished by using a lookup on the hashtable portion of the structure, inserting into both the table and the vector of data in the event of a lookup failure.
Finding the object for a key (the more common case) is works by indexing into the vector.

The only principal difference between this and a simpler design (a standard hash table mapping from the value to the key, and a vector mapping from a key to a value) is that it stores the data only once, and without any indirection.
While this may not sound like much, this gave a modest 23\% time performance boost over the standard library implementation in time, and approximately halved space on an earlier version of the use-after-free detector.
The closest approach still using the standard data structure would have been to use a smart pointer to share data between the data structures, or a hashtable of hashes.
The smart pointer caused trouble with the interfaces, and hashing twice incurred a performance penalty, so we used this solution.

\subsubsection{Aggregation}
As aggregation is described at the predicate level, we can implement it directly on the tuple storage.
Tuple storage is structured as a map from the tuple of non-aggregate fields (reordered to the front) to a tuple of aggregate fields.
These aggregate fields are represented by a triple of the value-keys to be aggregated, a current aggregate value, and an index indicating how many of the value-keys are aggregated in the cached value.
This allows for a lazily updated computation of the meet.

When a tuple is inserted into the store, if a value with the same non-aggregate fields is present, in which case the value-key list is extended, but the aggregate is left alone.
If it is not present, we initialize the aggregate value with the value of the key in that slot, fill in the key in the keys-to-be-aggregated, and set the index to 1.
When retrieving a tuple, we check whether the index is equal to the length of the comprising keys.
If it is not, we start the iteration at the index, and perform iterative meets until the aggregate is up to date.
We then return the tuple, extended by the aggregate fields and reordered.

\subsubsection{Join Computation}
Datalog computation is join-heavy, and as a result attempting to compute the join naively can lead to disasterous execution times.
There are a variety of existing join approaches.
% Cite postgres?
RDBMSes tend to favor straightforwards strategies, such as nested looping, hash join, and merge join.
Merge joins require a relevant index, but generally perform substantially better unless tables are extremely small.
Hash joins operate by creating an intermediate data structure of one of the tables which is indexed by the hash of the joined values.

However, for high-arity join patterns, better algorithms exist, usually formulated as ``worst-case join'' algorithms.
Ngo showed~\cite{nprr} that it is possible to develop join algorithms which are optimal even under these conditions.
This algorithm is rather complex, and is intended for theoretical results rather than actual implementation.
However, LogicBlox~\cite{logicblox} developed an algorithm known as Leapfrog Triejoin~\cite{lftj} which achieves these same bounds while remaining practically implementable over traditional indices.
Unfortunately, this algorithm is patented, and so could not be used.
This indicates a potential for future implementations to derive a novel approach from the AGM~\cite{agm} bound or Ngo's~\cite{nprr} approach, but developing such an algorithm is beyond the scope of this thesis.

In mycroft, we used a simultaneous merge join ordered from smallest table to largest table.
An index is selected for each table which walks it in unification argument order, with constant arguments being sorted to the front.
The first index is advanced to the first tuple where all the constant arguments match.
This is made easier by the use of integer-only tuples, as the non-constant arguments can be represented as 0 in a query to the index.
Then, candidate variable bindings are made to the unification terms (if possible) and the next table is considered.
When on the last table, if a candidate set of assignments to the unification terms can be completed, it is emitted and the index advanced by one step.
If the index cannot advance or the index fails to unify with earlier tables, we know that no further result is possible, and go back one table, and continue.
This approach keeps around only a small amount of additional state, linear in the number of clauses in the query, as it is returning results.

However, due to our need for \emph{incremental} results, we can improve this mechanism substantially.
Rather than computing the entire join at once, we split it into subjoins, one for each clause in the query.
We have a separate, much smaller index for ``new'' facts in referenced predicates, requested by the query at database initialization time.
We perform a subjoin with each predicate's large index swapped for this small index to get exactly those results which we would receive that we did not before, then chain them for a result.
The small indexes are emptied during this operation, so they will not yield the same results again.

As an example, consider evaluating the query $A(x, y) \& B(y, z) \& C(z, x)$ for incremental results.
The first time it is evaluated, we perform a full join, ignoring the subjoin strategy - it would be equivalent to performing the full join 3 times.
Then, we insert two facts into $A$, and one into $C$.
Running the query again, we perform three subjoins, one on $A', B, C$, one on $A, B', C$, and one on $A, B, C'$.
In our join algorithm above, remember that we sorted the smallest table to the left.
As a result, the join with $B'$ immediately terminates, yielding no results.
For the join with $C'$, it essentially acts as a join on $A$ and $B$ only, with a constant restriction.
The join with $A'$ is similar, but the $A'$ portion of the join yields two facts, so it essentialy runs two constant constrained joins of $B$ and $C$.

\subsubsection{Provenance Tracking}
In order to later manage circumscription, or to allow a human to trace the reasoning of a program, we need to keep track of where facts come from.
To do this, in conjunction with each tuple we store a list of possible justifications.
A justification is composed of the ID of a rule, and the IDs of the facts used to match the body clause of that rule.
An aggregation is represented simply as the list of fact IDs aggregated for the match.
A map is additionally maintained from fact IDs to justifications which contain them.

\subsubsection{Circumscription, Call/CC, and Retraction}
Implementing circumscription essentially involves monitoring accessed aggregations to see if they would change, and responding with a retraction.
The previous description of aggregates does not easily allow for this.
A tuple insertion does not know if something has depended on this aggregation's completeness, and if so what.
To deal with this, if a tuple is circumscriptively fetched, we replace list of merged keys in the aggregate field with a newly minted aggregate ID.
Three maps are maintained for aggregate IDs:
\begin{itemize}
	\item Aggregate ID to comprising Fact IDs
	\item Aggregate ID to dependent justifications
	\item Fact IDs to Aggregate IDs they comprise
\end{itemize}

If a tuple insertion occurs and would need to update an aggregate represented by an aggregate ID, that aggregate ID is retracted.
The retraction code acts as a worklist, initially populated by the broken aggregation.
First, it removes any justifications broken by the current retracted item.
Then, it retracts (by adding to the worklist) any facts which now lack justification.
If the current retracted item is a fact, it also retracts any aggregate IDs which now have one fewer fact.

In the special case where the tuple just inserted was also retracted, we replace its justification with one referencing the members of its now broken aggregate ID.
This ensures that while this justification no longer cares about the expansion of the circumscription, it will still be properly retracted if one of the facts in the original aggregate becomes invalid.

\subsubsection{Future Work}
% Index reduction
% LFTJ, tetris join
% Autotabling
% Threading
% Disk backed datastore
% Cow backed storage
% Disk backed tuplestore/indices
% Distribution
\section{Benchmarks}
