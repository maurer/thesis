\label{alias:sec:eval}
Our evaluation has 3 major components.
\begin{itemize}
\item Juliet - How do we perform on a labeled (true positives, false positives, false negatives) data set?
\item Real World Bugs - Can we detect real bugs? 
\item Ubuntu \texttt{\$PATH} - How often do we alarm on real bug-free code?
\end{itemize}

We evaluate over the Juliet test set both for comparability with other work~\cite{tac, juliet-eval-static-source} and to act as a baseline for our detection power and false positive rates.
It helps answer the question ``In the absences of confounding factors from the real world, how well does this work?''.
We also evaluate over real use-after-free bugs pulled from MITRE's CVE database.
Evaluating on these verifies that real world code, while potentially confounding, does not stop our technique from functioning altogether.
It also provides a measurement of the false positive rate in the presence of true positives.
Finally, we evaluate over a variety of believed-good binaries.
The intent behind this evaluation is to get a better idea of false positive rates and analysis costs for average programs believed to be non-buggy.

\subsection{Juliet}
IARPA released the Juliet test suite~\cite{juliet} as a way of providing standardized examples of CWEs.
By building only those corresponding to use-after-free, we get a high density test suite.

All three sensitivities find all intended bugs in Juliet.
Insensitive analysis generates 39834 false positives, reducing to 0 with flow sensitivity.
Run time was 19m30s for the flow sensitive version, and 30m4s for insensitive.
However, the insensitive variety generates its alias information as of 3m23s.
The system spent the remainder of the time generating reachability information.

Unfortunately, while Juliet serves as a good test for true positives, it does not do much to elicit false positives from our system, which is why both our performance and others' look too-good-to-be-true here.
While there are negative tests present, there is little in the way of things that checkers are traditionally weak against (data structures, recursion, loops, etc.).
For this reason, it is important to evaluate ourselves on real world code as well.

\subsection{Live CVEs}
\label{alias:sec:eval:real}
Our system successfully detects 7 real bugs across 6 programs.
All sensitivities of the checker detected all bugs.
We assume that all potential use after frees which do not match the known bugs in each of these programs are false positives.

\begin{figure*}
\begin{center}
\begin{tabular}{|c|c||c|c|c|c|}
\hline
Program & Sensitivity & Run time & Memory & False Positives & Binary Size\\
\hline \hline
\multirow{3}{*}{gnome-nettool} & Insensitive & 38s & 1G & 1851 & \multirow{3}{*}{156k}\\
	& Flow & 30s & 1G & 0 &\\
	& Flow + Ctx & 2m34s & 2G & 0 &\\
	\hline
\multirow{3}{*}{goaccess} & Insensitive & 4m35s & 15G & 387459 & \multirow{3}{*}{635k}\\
	& Flow & 16m14s & 10G & 112420 &\\
	& Flow + Ctx & 43m34s & 34G & 87 &\\
	\hline
\multirow{3}{*}{libarchive} & Insensitive & 1m23s & 3G & 4917 & \multirow{3}{*}{366k}\\
	& Flow & 34s & 1G & 852 &\\
	& Flow + Ctx & 22m12s & 44G & 7 &\\
	\hline
\multirow{3}{*}{shadowsocks-libev} & Insensitive & 2m12s & 5G & 130760 & \multirow{3}{*}{631k}\\
	& Flow & 3hr46m21s & 62G & 22357 &\\
	& Flow + Ctx & 3hr53m26s & 72G & 115 &\\
	\hline
\multirow{3}{*}{mdadm} & Insensitive & 16m45s & 31G & 1056570 & \multirow{3}{*}{768k}\\
	& Flow & 2hr24m13s & 42G & 270683 &\\
	& Flow + Ctx & 12hr10m43s & 111G & 14566 &\\
	\hline
\multirow{3}{*}{isisd} & Insensitive & 3m46s & 8G & 58241 & \multirow{3}{*}{451k}\\
	& Flow & 18m49s & 9G & 11776 &\\
	& Flow + Ctx & 22m32s & 25G & 513 &\\
	\hline
\end{tabular}
\end{center}
\caption{Real CVE Performance}
\label{fig:cveperf}
\end{figure*}

\begin{figure*}
	\begin{center}
	\begin{tabular}{|c||c|c|c||c|c|c||c|c|}
		\hline
		Sensitivity & \multicolumn{3}{c||}{Run time} & \multicolumn{3}{c||}{Memory} & \multicolumn{2}{c|}{Alarms} \\
		\hline
		& Avg & Median & Stdev & Avg & Median & Stdev  &Avg & Imp\\
		\hline\hline
		Insensitive  & 2m26.1s & 58.4s & 3m38.1s & 241.7M & 34.1M & 1.9G & 73.1 &\\ \hline
		Flow & 2m14.2s & 54.7s & 3m19.6s & 236.8M & 34.1M & 1.9G & 0.5 & 93.1\% \\ \hline
		Flow and Ctx  & 2m22.1s & 55.6s & 3m54.9s & 349.2M & 34.0M & 2.3G & 0.2 & 43.5\% \\ \hline
	\end{tabular}
	\end{center}
	\caption{Ubuntu \texttt{/usr/bin} Performance}
	\label{fig:ubperf}
\end{figure*}

Note that while the insensitive analysis completes quickly and cheaply for every binary, the false positive rates are so high that the output would be difficult to use.
Flow sensitivity reduces false positives significantly.
Manual analysis reveals that most remaining false positives are either due to data structure usage (which decreases the precision of the alias analysis), confused allocation sites from wrapped malloc constructors, and infeasible paths.
Context sensitivity gives additional improvements by helping to differentiate between instances of calling wrapped mallocs (e.g. \texttt{new\_foo()} to allocate and initialize a \texttt{foo}).

While performance for insensitive and flow-sensitive analyses appears similar, this is in large part because the generation of a global program reachability graph for each \texttt{free} is costly.
If the analysis is instead timed in phases, the alias-analysis-only portion for the insensitive system takes seconds, while it takes the bulk of the non-CFG-recovery time in a flow-sensitive analysis.

For the known-vulnerable set, flowsensitivity reduced the false positive set by an average of 90\%, and context sensitivity reduced it by an additional 84.1\%.
The false positive reduction for the addition of flow sensitivity is immense, and the increase in time and space needed for the alias analysis was manageable for programs in our known-vulnerable set, the largest of which was 768kb.
Adding context sensitivity furhter increased the time and space cost, but still yielded a major increase in precision.

\subsubsection{GUEB}
The author of the GUEB~\cite{gueb} tool made his tool open source\footnote{
	\url{https://github.com/montyly/gueb}
}, allowing us to compare against it.
We connected IDA, BinNavi, and GUEB and ran the system over the same bugs we evaluated against.
As a caveat, we could not feed them the same binaries our tool consumed - their tool's stack only accepts 32-bit, so we recompiled the same vulnerable programs in 32-bit mode.

Figure~\ref{fig:guebperf} shows the performance.
The crashes derive from unhandled cases in the input, and not fundamental to their methods.
The undetected bugs occur due to their choice to not follow back edges (either as recursion or loops) when computing their VSA.
This is an understandable choice, since VSA can become slow and be difficult to force convergence for when cycles are present in the input, but in this case it caused their analysis to miss bugs.
Likely due to this forwards-only approach, GUEB terminated rather quickly on all inputs.

In Listing~\ref{lst:isisd-gueb}, we can see one of the real vulnerabilities the lack of a fixpoint fails to detect.
The loop knows that \texttt{adj} is allocated and non-null on entry, so the first time through the loop is always fine.
However, some paths through the loop free \texttt{adj}, and go around the loop again.
At this point, a use-after-free can occur.
If back edges are not followed, the analysis cannot detect this.

\begin{figure}
\begin{center}
\begin{tabular}{|c||c|c|}
\hline
Program & False Positives & Bug Found? \\
\hline \hline
	gnome-nettool & 2 & Yes\\
	goaccess & crash & crash\\
	libarchive & 222 & Yes\\
	shadowsocks-libev & crash & crash\\
	mdadm & crash & crash\\
	isisd & 596 & No\\
\hline
\end{tabular}
\end{center}
\caption{GUEB Performance}
\label{fig:guebperf}
\end{figure}

\begin{lstlisting}[language=C, float=t, caption={\texttt{isisd} Vulnerability}, label=lst:isisd-gueb]
// ... (adj is allocated and constructed here)
for (level = IS_LEVEL_1; level <= IS_LEVEL_2; level++) {
	// ...
	else if (new_state == ISIS_ADJ_DOWN) {
		// ...
		isis_delete_adj(adj);
	}
}
// ...
\end{lstlisting}


\subsection{Ubuntu Path Sample}
Now that we know that our program will alert us to real world vulnerabilities, we also want to know how it will behave in the case where no expected vulnerabilities are present.
To this end, we ran our program across \texttt{/usr/bin} on a default Ubuntu Xenial installation, as shown in Figure~\ref{fig:ubperf}.

Adding flow sensitivity provided an average reduction in bug candidates of 93.1\% in those situations where the insensitive code found at least one candidate.
Then adding context sensitivity ($k$ = 1) reduced it by an additional 43.5\%, in those situations where the flow sensitive analysis had a bug candidate, and the context sensitive analysis terminated.

Manual auditing of the reported bugs did not reveal any true bugs, but did show that a common pattern amongst false positives was functions for whom one path freed and replaced a pointer, and the other did neither, and they rejoined.
A more aggressive analysis for dead variables could remedy this by pruning them to allow the freed region to leave the points-to relationship before the paths rejoined.

While we did not find any new bugs in this case, the system emitted a maximum of 22 reports on individual binaries (and this worst case had most of them clustered in the same code area).
This was few enough reports to enable practical manual auditing by a single individual.
This does not guarantee these programs are bug free - while we have a conservative analysis, that is dependent on seeing the entire control flow graph.
In this case this condition is not met.
Some C++ programs which use vtables are present in this path - calls to member functions there will appear as no-ops.
Function pointers are similarly considered to be no-ops.
Calls into libraries which were not analyzed with the binary are similarly absent.
Finally, some of these are GUI or threaded applications, which utilize a callback system we again do not handle control flow edges for.
