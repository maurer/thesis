\chapter{Background}
\label{chap:background}
% Binary analysis
\section{Binary Analysis}
% * Spiel about how binary analysis is different
We encounter new challenges when applying techniques from the source code world to compiled program analysis.
%   * Abstractions stripped out
Most of these challenges fall into the category of absent abstraction.
Function locations, variable locations and bounds, variable types, and control flow graphs are all generally taken for granted even in the analysis of ``low level'' languages such as C, but are absent in binaries.

%   * CFG unknown
The inability to recover a complete control graph can be one of the more difficult challenges to deal with.
Algorithms for recovering any of the other missing abstractions depend strongly on having a pre-existing control flow graph\cite{vsa,bddbddb,smash,tie,bitr,wrappedintervals,ramalingam2000}.
However, determining the potential targets of an indirect (computed) jump becomes more difficult without the abstractions present in the source language.
Being unsound here by asserting complete knowledge of control flow in cases where only partial knowledge is available can result in unsoundness in every downstream analysis.
Despite this, real systems frequently use ``good enough'' approximations due to the difficulty of this problem.

%   * Extraordinarily stateful, with large state
One of the features of functional code that makes it easier to analyze is its thorough reduction of state.
Traditional procedural or imperative languages make this slightly more difficult through the use of state variables which expand the set of implicit inputs to each step greatly.
Binary code is yet more difficult to analyze.
It becomes quickly difficult to determine what parts of memory and registers are really live without abstractions like array bounds or variable scoping.
It is difficult statically to know whether the process may access a given piece of state in the future.
The resulting problem size is larger because execution effectively ``leaks'' state that existing analysis cannot determine will be unreachable.

\subsection{Control Flow Recovery}
\label{sec:cfg}
The goal of control flow recovery is to generate a complete graph of possible program counter transitions in the program while containing minimal number of spurious edges.
The simple form of control flow recovery involves performing recursive descent disassembly from a given entry point and watching for a transition to a distinguished exit node (usually via a function return).
However, indirect jumps due to function pointers, C++ virtual method calls, and even creative compilation of case statements can wreak havoc on this method since the value of the jump target to is not known ahead of time.

In each of the above cases, the compiler would be able to accurately determine the range of jump targets based on its own code generation information and information present in the source code.
The type of a C++ object determines a small subset of legal function targets for a given method.
The compiler can determine where jumps which came from switch statements will go as they have explicit targets in the source.
Function pointers are the most difficult to resolve.
At the source level, the compiler at least has the ability to restrict the possible targets to functions of the right type.
Compilation strips away abstractions needed to perform any of these analyses.
This renders them useless for direct use on raw compiled code.

\subsection{Value Analysis}
\label{sec:valanal}
It follows that recovering over-approximations of the range of potential values for pieces of state in a program would be useful.
One common way to do this is to perform abstract interpretation of the program over a domain designed to model the range of values well~\cite{vsa,wrappedintervals,wrappedintervals2}.

Unfortunately, Value Set Analysis\footnote{
  Value Set Analysis is the current state of the art for value analysis over compiled code.
} (VSA) assumes that the control flow graph has already known in order to sequence its transformations.
Additionally, it can lose nearly all its precision by loops whose transformations are not well represented by its particular domain.
The state of the art approach to the co-dependence of the control flow recovery and value analysis problems is to iterate control flow recovery followed by value analysis.
This process allows the creation of new paths from new values, which in turn re-inform the value analysis~\cite{jakstab}.

\subsection{Type Recovery}
Another important static analysis is type recovery.
A binary program may not necessarily have a traditional assignment of types to variables.
However, source language requirements and programmer volition make it likely that there is a useful assignment of types.
Even assembly programmers and dynamic language users~\cite{jsinfer} usually use consistent types to some extent.

There are two primary sources for type information after the compiler strips abstractions: propagation from API/ABI boundaries and how the program interacts with the structure of the data.
Propagation~\cite{howard} of type information from code boundaries entails knowing a priori that a particular library or service in use by the program matches some signature, then following definitions of its arguments and uses of its results to annotate the program with this information.
This technique is straightforward and keeps useful information like type and field names. 
Unfortunately, it encounters difficulties with any kind of internal data structure.
Usage based analysis~\cite{tie,bitr} to perform better at recovering internal structures, and works by examining how the code defines and uses state and generating constraints for types based on that.

\input{alias/background}

% * Dynamic
\subsection{Dynamic Analyses}
Dynamic analyses are those which focus on properties of individual executions, or statements of the form ``there exists a trace with the property\ldots''.
These analyses are frequently good at pinpointing problems or increasing the accuracy of approximations.
However, being path oriented limits these techniques to describing what could happen, rather than what must always be the case.

%   * Fuzzing
\subsubsection{Fuzzing}
One common modern way to find issues in code is to simply perform a large number of executions with varied inputs.
This technique is known as ``fuzzing'', and is widely used in the search for security flaws.
Fuzzing is especially useful for analyzing compiled code as it does not rely on source-level abstractions.
The primary two defining axes of fuzzing are blackbox~\cite{peach,zzuf,rebert2014} vs whitebox~\cite{klee,godefroid2008,avgerinos2014} and generational~\cite{peach} vs mutational~\cite{zzuf}.

%   * Symbolic Execution + SMT
\subsubsection{Symbolic Execution}
A slightly more sophisticated (though more expensive) approach is to keep inputs ``symbolic'' while executing.
Essentially, the evaluator uses expressions based on input variables in the place of concrete values unless the evaluator requires a concrete value to take a step.
When a the evaluator encounters a conditional branch, it extends path constraint by conjunction with the condition.
This restricts the range of feasible inputs for this execution.
When the next instruction is a potentially dangerous operation (such as jumping to or dereferencing a symbolic value), a SMT solver can check a formula for the plausibility of the ``bad'' scenario\footnote{\
  SMT stands for Satisfiability Modulo Theory.
  It is a SAT solver whose variables have extra domains and additional operations.
  The SMT solvers discussed here operate over the theory of bitvectors.
}.
The execution engine constructs the formula by taking the conjunction of the path constraint with whatever condition suffices to cause the badly behaved event.
If the SMT solver returns a satisfying assignment to the formula, that solution describes a bug-causing input\cite{aeg,mayhem}.
If the formula passed is complete (the operation is \emph{only} dangerous if the formula is satisfiable), an UNSAT response from the SMT solver give safety for \emph{this path only}.
Since loops may create an infinite number of paths, it is difficult to demonstrate that any region is safe this way.
The major benefit of the symbolic execution and SMT approach is that it results in a concrete trace which exercises a bug condition.

% Logic Language
\section{Logic Language}
Logic languages are a family of declarative programming languages which express computation as a series of rules used to derive a conclusion.
This makes them well suited to the task at hand because it allows us to describe the provenance of information and to separate the execution strategy for a suite of analyses from the expression of said analyses.
We will focus our review on Datalog and Prolog as exemplars of two styles of logic programming.
% * Datalog
\subsection{Datalog}
\label{sec:fchain}
\subsubsection{Basics}
Predicates form the concept of a relation between multiple values.
For example,
\[
        \pred{parent}{\cdot, \cdot}
\]
represents a relation in which the left argument is the parent of the right argument.
%   * Facts
A ``fact'' is a particular member of a relation.
We use italics to denote a constant.
As an example,
\[
        \pred{parent}{\id{alice}, \id{bob}}
\]
represents the fact that $\id{alice}$ is $\id{bob}$'s parent.
The set of facts known is sometimes referred to as the database.
The database has two parts: extensional and intensional.
The extensional portion of the database is those facts which the program author or user provides to the system prior to execution; the intensional database consists of those derived from the extensional database.
%   * Rules
In order to actually perform computation with this system and create such an extensional database logic languages use ``rules'' which function similarly to inference rules from traditional proof writing.
To write rules, I'll use $\var{X}$ to represent a variable named x.
Following in the vein of the previous examples,
\[
        \frule{\pred{sibling}{\var{X}, \var{Y}}}{\pred{parent}{\var{P}, \var{X}}, \pred{parent}{\var{P}, \var{Y}}}
\]
expresses that if for some assignment to $\var{P}$, $\var{X}$, and $\var{Y}$, we know that $\var{X}$ shares parent $\var{P}$ with $\var{Y}$, then we can derive that $\var{X}$ and $\var{Y}$ are siblings.
The left hand side is the \emph{head clause} and represents the template used to build new facts with this rule.
The right hand side is the \emph{body clause} and describes the search or match which must have a result on the database.
An evaluation engine applies rules``to saturation''.
This means that it applies all rules repeatedly until the body clause no longer has any new matches in the database.\footnote {
Without external code, this is equivalent to ``the head clause is in the database, or the body clause does not match``. With external code however, the head clause is not necessarily predictable, so we instead describe each rule triggering at most once per assignment of variables which matches the database.
}
Given this, rule declaration order is irrelevant.
\footnote{
  Rule declaration order is irrelevant to results.
  It may cause different performance in some engines.
}
%   * Simple examples
\subsubsection{Example: Uncle}
If we ignore the genderedness of the term ``Uncle'', calculating from a series of parent/child relations all of the uncles is a straightforward example.
We begin with an extensional database consisting of:
\[
        \pred{parent}{\id{alice}, \id{bob}}
        \quad \pred{parent}{\id{alice}, \id{charlie}}
        \quad \pred{parent}{\id{bob}, \id{deb}}
        \quad \pred{parent}{\id{bob}, \id{ed}}
\]
We take the sibling rule from the earlier example, and also
\[
        \frule{\pred{uncle}{\var{X}, \var{Y}}}{\pred{sibling}{\var{X}, \var{P}}, \pred{parent}{\var{P}, \var{Y}}}
\]
which indicates that someone is an uncle if they are that person's parent's sibling.

We can compute the complete database from these rules and the extensional database.
The uncle rule cannot apply, since there are no facts for the sibling predicate.
Applying the sibling rule to saturation yields a database of
\begin{gather*}
        \pred{parent}{\id{alice}, \id{bob}}
        \quad \pred{parent}{\id{alice}, \id{charlie}}
        \quad \pred{parent}{\id{bob}, \id{deb}}
        \quad \pred{parent}{\id{bob}, \id{ed}}\\
        \pred{sibling}{\id{bob}, \id{charlie}}
        \quad \pred{sibling}{\id{charlie}, \id{bob}}
        \quad \pred{sibling}{\id{deb}, \id{ed}}
        \quad \pred{sibling}{\id{ed}, \id{deb}}
\end{gather*}
The right hand side of the sibling rule no longer applies, so the only option now is to execute the uncle rule.
Doing so to saturation yields
\begin{gather*}
        \pred{parent}{\id{alice}, \id{bob}}
        \quad \pred{parent}{\id{alice}, \id{charlie}}
        \quad \pred{parent}{\id{bob}, \id{deb}}
        \quad \pred{parent}{\id{bob}, \id{ed}}\\
        \pred{sibling}{\id{bob}, \id{charlie}}
        \quad \pred{sibling}{\id{charlie}, \id{bob}}
        \quad \pred{sibling}{\id{deb}, \id{ed}}
        \quad \pred{sibling}{\id{ed}, \id{deb}}\\
        \pred{uncle}{\id{charlie}, \id{deb}}
        \quad \pred{uncle}{\id{charlie}, \id{ed}}
\end{gather*}
At this point, the program has finished because neither rule applies, and so the program has finished.
It has also derived the only two uncle facts possible.
\subsubsection{Example: Related}
Alternatively, perhaps we only wish to know if two people are related.
Consider the rules
\begin{gather*}
        \frule{\pred{related}{\var{X}, \var{Y}}}{\pred{parent}{\var{X}, \var{Y}}}\\
        \frule{\pred{related}{\var{Y}, \var{X}}}{\pred{related}{\var{X}, \var{Y}}}\\
        \frule{\pred{related}{\var{X}, \var{Z}}}{\pred{related}{\var{X}, \var{Y}}, \pred{related}{\var{Y}, \var{Z}}}
\end{gather*}
We could structure the previous rule set as a two-step procedure: find the siblings, then find siblings of parents.
This rule set is potentially more interesting because it is a recursive build up of information.
However, this is not any more difficult than the previous query since Datalog applies its rules to saturation.
The first rule translates a parent/child relationship into a ``related'' relationship.
The second provides symmetry.
The third provides transitivity.
This is sufficient to compute the actual related set.
Execution proceeds much the same as the previous example, simply firing each rule until no rules are able to fire.
The only real difference is that the second and third rule will fire recursively.
%   * Termination
\subsubsection{Termination}
It might be surprising to realize that the language as described thus far is actually terminating despite allowing recursive rules.
Head clauses construct facts with variables or directly with values.
If constructed with a variable, the body clause must bind that variable.
If a body clause binds a variable to a value, that value must have been in the database.
As a result, the only possible values are those which were in the extensional database and those mentioned concretely in the rules.
With a finite domain of values and a finite list of predicates, there is a finite list of facts which any execution could instantiate.

Evaluation is a series of steps.
Each step fires the first rule it can find which matches, or if no rule matches, terminates.
Since this evaluation strategy is always productive or terminating, and we have established a finite upper bound on the database, we have a bound on the length of such an evaluation.
This evaluation strategy matches the saturatin/fixpoint semantics of Datalogs because it only produces facts according to the rules and only stops when no rules apply.

There are some important caveats to termination as a language feature.
%
Termination directly implies a limit to the language's power.
Datalog's inability to create new symbols can be limiting.
Unmodified datalog can still implement a number of different kinds of closure or search procedures, but even simple arithmetic is outside of its purview if not within a bounded domain.
%
Some language modifications (including those needed for \sysname) will introduce the ability to generate new symbols.
When this happens, the resulting language loses its termination guarantees.
\subsubsection{External Predicates}
\label{sec:extpred}
%   * Motivation for external predicates (e.g. +2)
One example of a case where introducing new symbols might be desirable is the ability to perform arithmetic.
It is possible to encode arithmetic over some finite domain (e.g.,\ $0-(2^{64} - 1)$) by adding a predicate for each binary operation and prepopulating the extensional database with all the values.
However, having such a table is massively impractical.

%   * External predicates
One way to deal with this issue is to introduce external predicates: predicates whose truth is not defined by presence in the database.
Instead, the engine computes or queries its truth from some external system when needed.
This leads to the need for a mode system.
Consider an external predicate $\pred{sum}{\cdot, \cdot, \cdot}$, where the first entry is the sum of the other two.
When trying to match the body clause of a rule, it becomes important that at the match algorithm sends at least two values to the external code; if only it only sends one, the resulting match set will have the enormous size of $|\mathbf{D}|^2$ where $\mathbf{D}$ is our domain set.
In the case of functions designed to be hard to invert (e.g.,\ a hash function), this becomes not only a result size concern and an actual implementability concern.
To deal with this, external predicates can have ``mode signatures''.
These signatures explain which entries are logical inputs and which are outputs.
For example, we could have
\[
        \pred{sum}{-, +, +} \quad \pred{hash}{-, +}
\]
A mode checking phase on the rule can then guarantee that matches against the sum predicate only needs an implementation of
\[
        \textrm{add} : \textrm{uint64} \rightarrow \textrm{uint64} \rightarrow \textrm{uint64}
\]
rather than needing to assume the presence of subtraction and possible-addends operations.

%   * External predicates can break termination
Unfortunately, adding external predicates adds all possibilities for their result values to the upper bound for the termination.
Adding an external predicate which only produces a small type such as a boolean has little effect on our ability to bound execution.
Adding a finite-but-large type like $\texttt{uint64}$ makes any numerical approximation of runtime from value count irrelevant, but still technically gives termination.
Adding a type with non-finite size like lists or strings \emph{breaks termination}.
As a simple example, consider an external predicate which appends the character `a' called `extend'. Then
\begin{gather*}
  \frule{\pred{infinite}{\var{X}}}{\pred{infinite}{\var{Y}}, \pred{extend}{\var{Y}, \var{X}}}\\
  \pred{infinite}{``''}
\end{gather*}
provides a simple infinite loop by extending the string with more `a's forever.

\subsubsection{Backwards Chaining}
\label{sec:bchain}
The evaluation model discussed thus far is what is commonly known as forward chaining, or bottom-up search.
This approach searches for a proof by taking known facts and deriving anything they can hoping to reach the goal.
However, there are some applications for which this approach does not work well --- as an example, any system that would have a potentially infinite intensional database would never complete execution with such a semantics.

Prolog uses the opposite approach: backwards chaining or top-down search.
In backwards chaining, a particular goal match directs the search rather than enumerating all possible derivations from available information.
The logic engine examines the goal and the available rules and breaks it down via unification to a series of potential subgoals, then recurses.

\subsection{Negation}
%   * Stratified negation
\subsubsection{Stratified Negation}
One interpretation of negation is as the inability of the system to prove a particular fact.
For example, this sort of negation could be useful to describe routing.
We can describe network reachability as:
\begin{gather*}
        \frule{\pred{reachable}{\var{X}, \var{Y}}}{\pred{link}{\var{X}, \var{Y}}}\\
        \frule{\pred{reachable}{\var{Y}, \var{X}}}{\pred{reachable}{\var{X}, \var{Y}}}\\
        \frule{\pred{reachable}{\var{X}, \var{Z}}}{\pred{reachable}{\var{X}, \var{Y}}, \pred{reachable}{\var{Y}, \var{Z}}}
\end{gather*}
Next, our machine has a local network and a gateway --- a common configuration.
Traffic serviceable by the local network should be directly sent, while the remainder of routable traffic should go to the gateway.
\begin{gather*}
        \frule{\pred{route}{\id{local}, \var{T}}}{\pred{reachable}{\id{local}, \var{T}}}\\
        \frule{\pred{route}{\id{gw}, \var{T}}}{\neg \pred{reachable}{\id{local}, \var{T}}, \pred{reachable}{\id{gw}, \var{T}}}
\end{gather*}
In the case of this set of rules, negation has a clear interpretation.
First, generate all reachability predicates.
Then, if reachable was not derivable thus far, matches against not reachable should succeed.
However, recursive rules and negation can mix to cause termination and stability issues.
Consider the set of rules:
\begin{gather*}
        \frule{\pred{p}{\var{X}}}{\neg \pred{q}{\var{X}}}\\
        \frule{\pred{q}{\var{X}}}{\neg \pred{p}{\var{X}}}
\end{gather*}
There is no interpretation of this which terminates with the fixpoint semantics, or which provides a minimal model~\cite{prologbook}.
To deal with this, Datalog imposes a requirement on negation known as stratification.
It is possible to phrase this requirement as a graph property:

Create a graph where the predicates are nodes with an edge between two nodes iff there is a rule in the program which uses the source node predicate as a premise to derive the destination node predicate.
Additionally, label all those edges where the with a negated premise predicate.
There must be no cycles which include one or more negated edges.

% * Circumscription
\subsection{Circumscription}
\label{sec:circumscription}
Circumscription is a concept in logic related to including Occam's Razor into deriving a model from logic statements.
We discuss it here because it will be relevant to the \sysname\ treatment of reasoning from incomplete information.

\newcommand{\true}{\textrm{true}}
\newcommand{\false}{\textrm{false}}
As an example of why circumscription might be useful, consider the following:
There are three cups marked $a$, $b$, and $c$.
There are balls in cup $a$, and at least one of cups $b$ and $c$.
This encodes to:
\[a \wedge (b \vee c)\]
We know the above formula is true, but not the truth values for $a$, $b$, and $c$, there are three possible satisfying assignments:
\[
        (\true, \true, \false), (\true, \false, \true), (\true, \true, \true)
\]
Circumscription is a mode of reasoning that in this situation allows us to examine the first two options rather than the third.
It is about picking the minimal situation that could correspond to what we know.

McCarthy's paper introducing the idea~\cite{circumscription} gives more examples.
A slightly simpler (if imprecise) way of thinking about circumscription is to consider it as taking a specification of truth which is irrefutable, while interpreting a minimal set of facts as true --- conclusions are not necessarily grounded in fact, but rather are statements without refutation.
%   * Original definition by McCarthy
\subsubsection{Original Definition}
Originally, McCarthy defined circumscription with respect to first order logic.
Specifically, the circumscription of a predicate $P$ to a predicate $Q$ in a formula $A$ is
\[
(A [Q/P] \wedge \forall x. Q(x) \rightarrow P(x)) \rightarrow (\forall x. P(x) \rightarrow Q(x))
\]
McCarthy calls this a ``sentence schema''.
Essentially, this says that for a given formula $A$ we know to be true, and some predicate $P$ we have incomplete information about, if some other (usually artificially defined) predicate $Q$ could replace $P$ in $A$ without changing its truth value, and $P$ is true whenever we know $Q$ to be true, then we can have the other half of the implication, making $Q$ a legitimate proxy for $P$.
Practically applying this usually involves declaring a $Q$ which is true in exactly those situations we know $P$ to be.
As a simple example, consider the formula
\[
        P(0) \wedge P(1) \wedge P(2)
\]
Define a predicate for circumscription $Q$ as
\[
        Q(x) \equiv (x = 0) \vee (x = 1) \vee (x = 2)
\]
essentially explicitly enumerating the cases in which $P$ must be true.
Circumscribing $P$ in the example formula using $Q$ yields
\[
        P(x) \rightarrow (x = 0) \vee (x = 1) \vee (x = 2)
\]
This transformation gives more information about $P$ than the original formula.
Specifically, we now know that $\neg P(4)$.

%   * Negation as circumscription yields stratified negation
\subsubsection{Relation to Logic Programming}
Circumscription is directly connected to modern logic programming through the negation features present in some logic language dialects.
We express the database and rules as a single logical formula by encoding as a conjunction of known facts as predicates applied to concrete values and rules encoded as implications.
Circumscribing over this formula will yield results consistent with a given logic program.
It will also allow us to potentially derive more by giving us an explicit definition of a predicate.
Circumscription provides the theoretical underpinnings for stratified negation.
For example, take the system
\begin{gather*}
        \pred{square}{\id{w}} \quad \pred{square}{\id{x}}\\
        \pred{silver}{\id{x}} \quad \pred{silver}{\id{z}}\\
        \brule{\pred{circle}{\var{X}}}{\neg \pred{square}{\var{X}}}\\
        \brule{\pred{copper}{\var{X}}}{\neg \pred{silver}{\var{X}}}\\
        \brule{\pred{penny}{\var{X}}}{\pred{copper}{\var{X}}, \pred{circle}{\var{X}}}\\
\end{gather*}
If we ask ``Is $\id{y}$ a penny?'', i.e.\ $\query{\pred{penny}{\id{y}}}$, the system without circumscription cannot answer yes.
The system cannot derive that $\id{y}$ is circular because it neither knows that a priori, nor does it know that it is non-square.
Circumscribing over both $\pred{square}{\cdot}$ and $\pred{silver}$ in the system allows us to derive $\neg \pred{square}{\id{y}}$ and $\neg \pred{silver}{\id{y}}$.
In this way, circumscription forms the basis for the form of negation present in logic languages.


