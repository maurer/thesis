\subsection{List Summation Example}
\label{subsec:workedexample}
First, we demonstrate \bitr\ in action. We use a \texttt{loop\_sum} function, shown in Figure~\ref{fig:loop-sum-c}, which sums the elements in a list. Figure~\ref{fig:loop-sum-asm64} shows the disassembly of the compiled code. We have replaced all the addresses with an incrementing label to enable us to reference instructions more easily in this example.
First, we lift the compiled code to BIL, then lift that to SSA, and finally run simple optimizations on the result to make it more readable. This leaves the code as in Figure~\ref{fig:solvedlist}(Appendix), but without the annotations. This diagram also contains an embedded disassembly showing where the code came from.

\begin{small}
\begin{figure}
\begin{lstlisting}[language=C]
struct list {
  struct list* next;
  int v;
};

int loop_sum(struct list* l) {
  int v = 0;
  while (l != NULL) {
    v += l->v;
    l = l->next;
  }
  return v;
}
\end{lstlisting}
\caption{Loop Sum C}
\label{fig:loop-sum-c}
\end{figure}
\end{small}
\begin{small}
\begin{figure}
\begin{lstlisting}[language={[x86masm]Assembler}]
loop_sum:
00:       push   %rbp
01:       mov    %rsp,%rbp
02:       mov    %rdi,-0x18(%rbp)
03:       movl   $0x0,-0x4(%rbp)
04:       jmp    11
05:       mov    -0x18(%rbp),%rax
06:       mov    0x8(%rax),%eax
07:       add    %eax,-0x4(%rbp)
08:       mov    -0x18(%rbp),%rax
09:       mov    (%rax),%rax
10:       mov    %rax,-0x18(%rbp)
11:       cmpq   $0x0,-0x18(%rbp)
12:       jne    5
13:       mov    -0x4(%rbp),%eax
14:       pop    %rbp
15:       retq   
\end{lstlisting}
\caption{Loop Sum Assembly (64-bit)}
\label{fig:loop-sum-asm64}
\end{figure}
\end{small}

The first thing a reverse engineer would do is to identify that the program uses \texttt{rsp} as the stack, see the standard function prologue in instructions 00 to 01, and skip past the prologue. At this point instruction 02 loads \texttt{rdi} into stack slot $-0x18$, and remember that this stack slot has a type corresponding to the first argument of the function, assuming that normal AMD64 calling conventions are in use. Instruction 03 then initializes stack slot $-0x4$ to $0$. However, since the write is 32 bits wide and not a 64-bit wide write, we immediately know the stack slot $-0x4$ is not a pointer. So, we assume stack slot $-0x4$ is an integer. Next, following the jump from 04 to 11, 11 compares $0$ against stack slot $-0x18$, the one that contains the first function argument.

Along the inequality case in 12, the next instruction is 05. This branch dereferences slot $-0x18$, adds 8, and dereferences it again. Now we know that the input must be a pointer, with \emph{something} at offset 8. Finally, we note that the width of the read is 32 bits, so we again have the situation from earlier --- this is an integer. At 07, the code adds the value read out back into stack slot $-0x4$. 08-09 dereference slot $-0x18$, so we know that in addition to pointing at something at offset 8, stack slot $-0x18$ also points to something at offset 0. Instruction 10 moves that value into stack slot $-0x18$! We now know that in addition to having a 32-bit integer at offset 8, at offset 0 stack slot $-0x18$ will have a value the same type as itself. If we assume the branch where our new pointer is null as we traverse 11 and 12 this time, we see that the value at stack slot $-0x4$ is the one returned. As a result, we now know that this function takes a linked list of integers as an argument, and returns an integer.

Now, this is roughly how a human would solve the problem. There are heuristics (e.g. this branch is feasible because the value is nonzero, then the code dereferences this variable, so this variable is a nullable pointer), shortcuts (ignore the function prelude), and tricks (e.g. treating stack slots as variables) that the human used which are not ideal for automated analysis. Branch feasibility is potentially expensive, function preludes are varied, and stack-slots-as-variables is confused by use of stack addresses in pointer math or function calls. However, there a number of tricks in that narration which are relevant to computers. For example, when we found something out about a value in a register, and the register came from stack slot $-0x18$, we would not just assign the property to that register, we would also assign the property to the stack slot, and the function argument. This process that enabled us to discern that the struct had at least two accessed fields. Additionally, we noticed that there was dataflow from a variable into itself, and used this to realize that the input variable must contain a recursive type.

We will now approach this in a more mechanized way, similar to how our reconstruction system would attack the problem. We omit some steps for brevity, but the reasoning will follow the same basic procedure. Assume for notation that $\tau(v)$ is a function that grabs the type variable corresponding to a variable. First, we go about generating constraints for each BIL statement. For example, when we subtract 8 from the initial \texttt{rsp}, we have two possibilities --- either \texttt{rsp} was an integer, in which case we're doing arithmetic and would want to generate
$$(\tint{64} \subt \tau(\textrm{R\_RSP\_211})) \wedge (\tau(\textrm{R\_RSP\_328}) \subt \tint{64}))$$
However, if we have a pointer, we instead want something along the lines of
$$(\tau(\textrm{R\_RSP\_328}) = \ptr{0}{0}) \wedge (\tau(\textrm{R\_RSP\_211}) = \ptr{0}{-8})$$
(0 is a fresh region variable in this example). Since we do not know which operation the instruction represents at constraint generation time (i.e. which arm of the intersection type for the addition operator is actually in use), we take the disjunction of these choices to get the total constraint. The next statement is a store; the choice of operations is unambiguous:
\begin{align*}
& (\mem \subt \tau(\textrm{mem64\_212})) \wedge& (\tau(\textrm{mem64\_327}) \subt \mem)\\
\wedge &(\tau(\textrm{R\_RSP\_211}) = \ptr{1}{0}) \wedge& (1 : (0 : 2)) \wedge (\tau(\textrm{R\_RBP\_0}) \subt 2)
\end{align*}
The region variable 1, and type variable 2, are both fresh here. The rest of the constraints can be derived in similar fashion.

Now that we have this list of constraints, how are we going to solve them? The difficulty lies in these optional clauses, so we handle those last. We can start maintaining sets of type variables which form equivalence classes to make sure the information gained constrains to all of them. We can bound these equivalence classes above and below. We can do the same for region variables, but keeping track of offsets.

Actually trudging through this would take a while, but we can focus on seeing that \texttt{rdi} in this example must contain a recursive reference. \texttt{rdi} corresponds to type variable 16 in the constraint list provided. Selecting non-optional constraints first, and resolving the type variable unification and region variable unification constraints arrives at this in relatively few steps. The computer, when doing this, does not follow a pattern with a goal in mind like this. The inference engine just goes through the constraints which it can consistently absorb one by one, generating a \emph{context} which contains a reduced form of the constraints.

Though much less practical for the manual reverse engineer, this method is vastly more mechanizable and less brittle. Effectively, to finish the example, the reverser would keep adding to their understanding, consuming constraints, until a disjunction makes a choice necessary. At that point, the reverser would bookmark their current understanding, and try one of the choices. If a given path did not work out, the reverser would backtrack to another choice. The key here is that incorrect choices will fail quickly; if the reverser tries to pick the integer math option on an addition, and the operation was really pointer arithmetic, by absorbing the non-choice constraints first, it will instantly see a contradiction in the accumulated knowledge of the situation.

Figure~\ref{fig:solvedlist} shows the complete result of running this algorithm in full on the \texttt{loop\_sum} example. That representation, while more complete, is quite complicated, so here we instead display the recovered types of a few registers at function entry. The stack ($\texttt{esp}$) at function entry:
\begin{align*}
*&(-32 : \trec{\textrm{A}}{*(0:\textrm{A})(8:\tint{32})})\\
&(-12 : \tint{32})(-8:\tpoly{B}{\bot_{64}}{\top_{64}}{B})
\end{align*}
The -8 position on the stack is to contain the old value of $\texttt{rbp}$. Its value is never used or defined, so the algorithm does not discover anything beyond its size. At position $-12$, we see our first local variable. We know from manual inspection that this contained the running sum. It has been inferred to be a 32-bit integer, as we would hope. Lastly, at $-32$ we see the local pointer the function used to iterate through the linked list - it contains a next pointer at offset 0, and a 32-bit integer payload at offset 8.

Examining $\texttt{rdi}$ at function start gives us our input type, since this function was built with the normal C 64-bit calling convention:
$$\trec{\textrm{A}}{*(0:\textrm{A})(8:\tint{32})}$$
This gives us what we expected - the function is taking in a linked list of 32-bit integers. Finally, we examine $\texttt{rax}$ at function exit to see what it returns, and see that it contains a padded $\tint{32}$
