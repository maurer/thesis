%Design
\section{Inference Method}
\label{sec:infmeth}
% Short blurb saying why implementation is interesting
There are two major components in the inference of types for a piece of code. First, we generate type constraints based on the action of the code itself, with every update to a register assigned its own type variable. BIL statements which have multiple possible meanings (an add that could be a numeric or a pointer operation for example) generate a disjunction constraint. Then, we solve these constraints, and use the now-known types for each type variable to form the solution. The constraint generation phase occurs on SSA-form BIL, as generated by BAP~\cite{bap}.
This allows the constraint generation to use the use-def information built into SSA, allowing separate constraint generation for each statement.
The constraint solving is the more difficult part, and uses an extended form of unification in order to transform the constraints into a set of conservative conditions on the type of a given register definition.
The choice of which constraint to satisfy in each disjunction effectively specifies what operation would a decompiler would select in the translation to a typed language without intersection types~\cite{Jim1995,Shao1993}\footnote{
	Intersection types represent a more general type for values which can have multiple possible types which are not partially ordered on the subtyping lattice.
}.

% Overview: describe the different parts
\input{bitr/example}

% Subsec for each part
\subsection{Sufficiency of Register Types}
\label{subsec:regonly}
Previous work~\cite{tie,sw} has used some form of variable recovery before attempting to regenerate types in order to avoid dealing with storage locations whose type will change as the program executes. TIE used methods from DIVINE~\cite{divine} to find its list of variable locations, while SecondWrite depended on LLVM's \texttt{mem2reg} pass. As SecondWrite mentions, DIVINE is slow, and thus poorly suited to large scale analysis. SecondWrite's choice of \texttt{mem2reg} is much faster, but any nontrivial use of a stack address will prevent that slot from promotion to a variable, and therefore prevent its analysis. Instead we opt to avoid the notion of variable recovery during our type recovery. Any access to a variable must either be through one of the available registers, or via a prearranged, well-known address (i.e. for a global). As a result, if we track the types of registers, including the fields of their structures, we recover the types of the variables without even considering which areas were originally variables and which are not until evaluation. This also removes dependence on assumptions of variable access patterns, and provides a more direct view of the types at the assembly level. With  minor tweaks at function call boundaries, even the stack bears representation as another struct pointer. This major insight allows us to avoid dependence on potentially expensive or fragile analyses as preconditions for our inference.

\subsection{Constraint Generation}
%SSA means we can examine statements separately
Using a SSA-based representation means we can examine statements separately, as the transformation encodes the dataflow problem in the naming. The constraint generation does not need to ask whether this \texttt{eax} and that \texttt{eax} are the same, as the variable names identify a unique definition site. As a result, we do not need to consider the context in which a statement occurs in order to generate the constraints for that statement.
The constraints will interact with other constraints, but this will dispatch on type variable matching rather than control flow.
This greatly simplifies this step.
\subsubsection{Constraint Forms}
\newcommand{\constr}{\textrm{constr}}
\newcommand{\sconstr}{\textrm{stmt constr}}
\newcommand{\pconstr}{\textrm{program constr}}
\newcommand{\unify}{\cong}
\begin{figure}[t]
\begin{align*}
\constr ::=&\, A \subt \tau
          |\, \tau \subt A
          |\, A \subt B\\
          |&\, A \unify \tau |\, A \unify B\\
          |&\, rv : \rho\\
          |&\, \constr \wedge \constr\\
\sconstr ::=&\, \constr\\
          |&\, \sconstr \vee \sconstr\\
\pconstr ::=&\, \sconstr\\
          |&\, \pconstr \wedge \pconstr\\
\end{align*}
\caption{Constraint Grammar}
\label{fig:cform}
\end{figure}

%Kinds of constraints
We can constrain our unifier based on the statements defining and using variables. First, we can apply upper and lower bounds to a type variable. This expresses that in whatever solution we come up with, the type variable must fall in a given range. For example, if the program assigns a pointer to a variable, its type must be above the pointer type. Similarly, if the program jumps to a variable, that variable must fall below $\code$.
We represent these restrictions as $A \subt \tau$, $\tau \subt A$, or $A \subt B$.
Notably, a type variable must be alone on at least one side of the constraint. We were able to express all the expression and statement constraints in this form.
Disallowing statements of the form $\tau \subt \tau'$ made implementing the solver easier due to the ability to index any constrained entity by a type variable rather than needing the ability to break down both sides simultaneously to make a the original constraint hold true.

Additionally, we have constraints for explicit unification, of the form $A \unify \tau$ or $A \unify B$. Again, we intentionally did not allow $\tau \unify \tau'$ for simplicity. This constraint indicates that we somehow either know the exact content of a type variable, or that two type variables really refer to the same thing. This is primarily useful for dealing with assignments to structures, where we want to merge the constraints accumulated so far on fields of two regions discovered to be the same. For most purposes, unifying a type variable with a type is the same as applying an upper and lower bound of that type to the type variable. The one exception to this is for pointers, which due to their subtyping structure, will not unify their regions unless an exact match on all defined offsets is present.

The last kind of constraint is the unification of region variables with regions, written $rv : \rho$. This kind of constraint requires that those fields defined in $\rho$ are in the solution for $rv$, and unify with the type variables in those fields in $\rho$. This allows conveniently constraining portions of a structure type at a time.

By taking the conjunction of constraints formed like this, we can express any particular interpretation of a statement.
%Each statement creates a disjunction of conjunctions
However, one of the unique parts of this particular typing problem is that some of our functions (especially +) have multiple interpretations which cannot be conveniently formed into a single type. For example, it is unclear whether $x$ is a number or pointer in the expression $x + 2$, and as a result, the type of $x + 2$ is unclear.
This forces us to either consider intersection types~\cite{Jim1995,Shao1993} or use a disjunction of constraints per statement. In the intersection typing approach, the system generates as complete a type as possible for each variable. For example, if we were trying to type a function $f(x) = x + 2$, the type would be similar to $f : (\mathrm{int} \rightarrow \mathrm{int}) \wedge (\mathrm{ptr}(r@n) \rightarrow \mathrm{ptr}(r@(n+2)))$. While this approach initially seems more elegant, as it allows more complete descriptions of a piece of code, even inference of well-behaved code can quickly become exponential both in the time taken, and in the size of the result type. Instead, we opt to generate a disjunction of constraints for statements which include expressions which would require intersection typing for a most general type. As long as we are satisfying one constraint, the expression will be legal, and in practice, operations like $+$ are not used for different purposes in the same generated code. This approach leaves out some possible typings (e.g. if whether a variable is a pointer or an integer is unclear, the system will end up needing to select one) and makes a small number of programs no longer legal (for example, a non-builtin plus function used both for pointer arithmetic and for integer math). Additionally, the choice to use constraints will make our search problem (in terms of finding which interpretations work) more tractable than inference would be in the intersection typing case. As a result, a statement's constraint ($\sconstr$) is a disjunction of conjunctions of the core constraint type ($\constr$). In order to describe the entire program, we take the conjunction of all the statement constraints and solve the result.

As each of these constraints are separately generated, in a Datalog-based system we could express their generation as an external predicate on a single rule.
This rule could generate a separate fact for each possible disjunction for an IL statement, or a single no-op constraint for instructions which would normally not have yielded one.

\subsubsection{Inter-procedural Constraints}
%Inter-procedural
The basics of inter-procedural analysis are straightforwards in this system. We use unique variables when lifting each function and store them in a table. On a function call, we look up the target function's input registers, and say that their types must be a supertype of the type of those same registers at the event of function call. Next, we process the output registers similarly, applying subtyping constraints here instead. There are two issues here, both deriving from the stack: pointer super/subtyping and stack slot re-use. The first issue occurs when applying a supertyping to the stack register upon a call. The stack register's struct will then include temporaries from the callee.
This will work fine, until two functions calls occur in sequence with incompatible local stacks.
This will cause an issue because the stack is now constrained to have incompatible uses of stack slots beneath the caller's stack.

The second issue is that compilers will commonly re-use stack slots for calling functions, so if the program calls functions with incompatible inputs in sequence, the stack will be ill-typed. The easiest approach is to simply assume that the stack register has lost all meaning post function call, and re-infer the relevant portions based on its use after that. However, this will be dropping potentially useful information. A slightly more sophisticated approach would be to try to identify function call prologues and pull the stack type from before them. Unfortunately, that approach would be compiler specific.

Instead, we add the ability to erase type variables from regions after a function call occurs. For example, on an i386 system, calling a two argument function will cause the bottom three values on the stack (e.g. including the return pointer) to no longer be present in the type of the stack post call. This is one of the few convention-specific adaptations of the system; the function call ABI is agnostic, as the convention definition is just a description of what registers each call uses/defines, and what registers the convention uses for input and output. However, in order to deal with stack-based calling convention, we have adapted to the notion that the stack pointer has a special set of invariants at calls.

\subsubsection{Examples}
%Examples
One example of a simple constraint generation would be for a multiplication. If we have the a fragment of code in the IL $X_1:64 = X_0:64 * 3$, we are dealing with the simpler case of a non-intersecting expression. We know the exact bit-width of $X_0$ and $X_1$ from the lifting process. Assuming that $X_0$ and $X_1$ correspond to the type variables $\tau_0$ and $\tau_1$ respectively, we would generate the constraint $(\tau_0 \subt \tint{64}) \wedge (\tint{64} \subt \tau_1)$. Note that these constraints are only one-sided subtypings. $X_0$ is only constrained to be usable as an integer. For all we know $X_0$ could have been a pointer, and this would still be legal. $X_1$ on the other hand must be definable by an integer. As a result, if $X_1$ is later dereferenced, the system will find this to be inconsistent. In a slightly more complex example, we examine $Y_1:64 = Y_0:64 + 8$ in a system with 64-bit pointers. In this case, this statement needs to generate two possibilities --- one assuming that addition is an operation over integers, and one assuming the addition describes pointer arithmetic. Assuming type variables similar to previous example, we end up with a constraint $((\tau_0 \subt \tint{64}) \wedge (\tint{64} \subt \tau_1)) \vee ((\tau_0 \subt \lptr{rv}{0}) \wedge (\lptr{rv}{8} \subt \tau_1))$. This describes both the pointer structure indexing behavior and the integer behavior simultaneously. Later, when trying to solve the constraints, we will have to select one of these behaviors. In the actual system, we also must cope with the possibility that $8$ is an array index, so we add yet another constraint to the disjunction.

\subsection{Unification}
Unification is the process of coming up with a valid substitution for a set of type variables such that the substituted system will satisfy some set of constraints. Normally, these are only equality constraints. However, in our system we are simultaneously solving regular unification constraints and subtyping constraints to come up with a substitution for each type variable that will satisfy not only the equality constraints, but also the subtype ranges. In order to do this, we maintain a context that tracks which constraints we have absorbed, maintains a simplified form of constraints, and allows for efficient checking of whether the context is still consistent.

%Subtype
The simplest kind of constraint is a lower or upper bound. If the constraint is entirely abstract (e.g. both are type variables, and neither type variable has a known substitution) then we just record the relation into our context for consistency checking as we load other constraints. If one bound is concrete, we load that bound into the type variable's constraint in the context, taking a meet or join as necessary. If both bounds are concrete, we check that the two types are subtypes, possibly propagating requirements to the context if both are pointers.
In the special case where the constrained types are pointers, we want to delay processing of this constraint until the context has processed the rest of the constraints.
The reason for this is that we need all of the offsets defined on the pointers that ever will be in order to propagate them across during this. In practice, these constraints tend to match function calls, and so running them last is usually a good decision; each function is usually understandable on its own.

%TVTVUnify
When two type variables must be equal by a constraint, if both are abstract, the context merges their bounds, and one of their equivalence classes chosen as the representative for both. If only one is concrete, the system checks that the determined type matches concrete bounds (e.g. bounds which are types) and then takes all the bounds which are on type variables, and sends them to the corresponding type variable. For example, if we are unifying $\tau_0$ and $\tau_1$, and we know $\tau_0$ is a $\tint{64}$, and $\tau_1 \subt \tau_2$ in the context, then we might end up picking $\tau_0$ as the representative for $\tau_1$, deleting $\tau_1$'s constraint entry, and adding $\tau_1 \subt \tau_2$ to $\tau_2$'s constraints. If both type variables are concrete, the system verifies equality, unifying argument regions in the case of pointers.

%RVUnify
During pointer unification, region variables will need unification. First, we need to make sure that for each element in the region, we unify those type variables. Then, we need to select one region variable to be the representative for all the equivalent region variables. Notably, since all pointers are modulo offsets, each region variable also needs to know its offset from the representative. Finally, we update that region with all the type variables that had definitions in one but not the other. If we want to unify a region variable with a sample region, the core operation is to provide a set of type variable unifications for some subset of offsets.

The primary design issues here are to avoid cycles in updates when there are cycles in the types, and to track only the relevant parts of the constraints (effectively reducing them). This allows us to efficiently check whether or not we have violated constraints in order to ensure that our search through the possible disjoint constraints can take place efficiently.

%Datalog repr
This operation, when implemented in Datalog, would need some care to avoid excessive redundant computation.
When adding two constraints to the solution context, the order should not matter if both of them would succeed.
Specifically, they should commute.
Additionally, if we have solved some set of constraints together, we do not wish to attempt to solve any subset.
These two properties together suggest a lattice-like aggregation structure.

\subsection{Search}
% Necessary due to disjunctions
As we alluded to before, instead of dealing with exponentially sized types, we choose to use constraints which were potentially disjunctive. Unfortunately, having disjunctions in our constraints means we have to make choices when attempting to unify them. This forms a sort of search problem where for each statement, we want to select the statement that will lead us to a valid unifier, if possible. Initially, this seems worrisome, as there are a potentially exponential number of choices. Luckily, as alluded to in our discussion of how to absorb a constraint into the context, we can cheaply check for correctness in partially inferred contexts. As a result, we can make a choice, and then if the choice is wrong, stop before we have spent time dealing with the whole path. This, combined with the desire to only require a single path, not all paths, reduces what would naively be an exponential process to a tractable one.
% Note: Assuming I fix the bugs I have, runtime will be O(nlogn), at the moment, runtime is O(n^2logn) which is nothing to write home about, thus the use of "tractable"

% What to do when not all satisfiable
Unfortunately, not all programs are typeable. This can occur for a number of reasons, including the program doing something that is actively unsafe, unmodeled operations, and lifting or control flow analysis errors. A good type recovery algorithm needs to be robust in the face of this, so we need some goal for what to do if the constraints are not simultaneously satisfiable. We choose to select an answer which satisfies the maximum number of constraints. In the degenerate case where all constraints are simultaneously satisfiable, the satisfying solution is still the best one.
In situations where the constraints are not simultaneously satisfiable, this goal corresponds to assuming we misunderstood the meaning of a minimum number of statements.

% Strategy
First, we sort the constraints by the number of disjunctive clauses. This means that in the case where the constraints are satisfiable, the solver processes the non-branching constraints first. This is both positive from a search perspective (early decisions are less likely to be wrong), and from a domain specific perspective, as pointer reads and writes are of this form. Processing pointer reads and writes early means that incorrect choices for pointer arithmetic are likely to fail immediately.
Then, for every constraint, we process each disjunction into a separate possible context. We then score each context with a triple of the number of constraints possibly satisfied, the number of constraints already satisfied, and a tie-breaker value.
In each of the constraints, the disjunction ordering matches how probable the interpretation is. For example, doing array indexing by a constant is less likely than doing struct indexing by a constant.
We base the tie breaker on the combined likelihood of each disjunction choice in a vacuum.
At each step, we take the current context under examination, grab its next constraint, then for each choice, or the choice of dropping the constraint, generate the possible next steps, and place them into a heap. The sorting order for the heap is first by constraints already processed (to avoid backtracking when we do not need to), then by possible constraints to solve (to ensure we will search for the best solution first), and finally by the tie breaker, to prefer constraint choices that are more likely a priori.

\subsection{Non-Monotonicity}
\label{bitr:sec:circ}
% Datalog link
Searching for a minimum number of dropped constraints again informs Holmes design.
Enabling the arbitrary dropping of constraints in a Datalog representation would result in an intermediate state too large to deal with.
Even if dropping only one constraint is sufficient, a program written in this way would compute the potential dropping of every possible constraint, resulting in an enormous set of facts.
In order to make this more practical, we would either need to introduce control flow primitives (such as Prolog's cut) or some form of non-monotonic reasoning.
Assuming no explicit control flow primitives, we require non-monotonic reasoning because adding a new arm to a disjunctive constraint would add a fact, but possibly \emph{reduce} the number of facts which a correctly designed system would derive due to decreasing the number of dropped constraints.

This form of non-monotonicity matches circumscription in combination with the call/cc feature.
We can structure a maximum number of dropped constraints as a closed world hypothesis (circumscription), making an assumption that the number of constraints to drop will not increase.
Then, if for a given number of maximum dropped constraints, it can determine that no complete solution can exist, the maximum number of dropped constraints can increase (call/cc), retracting the insolubility assertion in the process.

%  Limitations
\subsection{Limitations}
\bitr\ does not implement every possible type, or understand every form of invariant. For example, \bitr\ does not know how to deal with union types, even if a tag indicates which type the variable is. We could extend to deal with such types, but doing so would make the system a good deal more complicated, and move from being only dataflow dependent to being control flow dependent as well. Additionally, \bitr\ does not analyze value bounds on types, as one might expect from an enumeration type or an array index. Adding this form of analysis would require more detailed understanding of the numeric operation, and would require analysis resembling VSA\cite{vsa}.

Another issue is in the use of functions with variable arity. In order to do inter-procedural analysis, \bitr\ matches the input and output registers together for unification. However, without separate instantiation of the function at each call site, the varargs portions of the function's input stack will not match across usage of the functions. If functions were separately instantiated however, information from each call site would not propagate to another. It would be possible to write code that special cased the varargs on x86 calling convention, but this is specialization and future work.

Finally, the more inconsistent the program, the longer the system will take to recover the types. Especially nonsensical programs can take a long time as the system attempts to optimize for the fewest number of broken constraints.
